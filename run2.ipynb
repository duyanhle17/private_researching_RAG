{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip install -q faiss-cpu sentence-transformers langchain-text-splitters rouge numpy scipy spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List, Union\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th·ª≠ (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NVAPI_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th·ª≠ (2) NANO graphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SimpleGraphRAG:Loading embedding model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:SimpleGraphRAG:Loading spaCy model...\n",
      "INFO:SimpleGraphRAG:Computing embeddings ...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.31it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.07it/s]\n",
      "INFO:SimpleGraphRAG:Creating FAISS index (dimension=384)\n",
      "INFO:SimpleGraphRAG:FAISS index built\n",
      "INFO:SimpleGraphRAG:Saved embeddings to ./nano_graphrag_cache_2026-01-20-10-37-23/embeddings.npy\n",
      "INFO:SimpleGraphRAG:Building Knowledge Graph (chunk-based NER)...\n",
      "INFO:SimpleGraphRAG:KG built with 5852 nodes, 8579 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved KG to sat_data/kg.pkl\n",
      "Nodes: 5852\n",
      "Edges: 8579\n"
     ]
    }
   ],
   "source": [
    "# simple_graphrag.py (upgraded -> v3.0)\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Set, Tuple, Dict, Any, Optional, Callable, Union\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from rouge import Rouge\n",
    "# import google.generativeai as genai  # Gemini SDK\n",
    "import os\n",
    "os.environ[\"NVAPI_KEY\"] = \"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\"\n",
    "# 3) Create a client wrapper\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVAPI_KEY\")\n",
    ")\n",
    "\n",
    "# Optional / extra libs (used if installed)\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception:\n",
    "    tiktoken = None\n",
    "\n",
    "try:\n",
    "    from node2vec import Node2Vec\n",
    "except Exception:\n",
    "    Node2Vec = None\n",
    "\n",
    "# Optional clustering libs\n",
    "try:\n",
    "    import igraph as ig\n",
    "    import leidenalg\n",
    "except Exception:\n",
    "    ig = None\n",
    "    leidenalg = None\n",
    "\n",
    "# try:\n",
    "from networkx.algorithms import community as nx_community\n",
    "# except Exception:\n",
    "#     nx_community = None\n",
    "\n",
    "# ---------- Logging ----------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"SimpleGraphRAG\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Utility LLM (Kimi K2) ----------\n",
    "# async\n",
    "def llm_generate(prompt: str,\n",
    "                       model: str = \"moonshotai/kimi-k2-instruct-0905\",\n",
    "                       temperature: float = 0.2,\n",
    "                       max_tokens: int = 1024):\n",
    "    \"\"\"\n",
    "    G·ªçi Kimi-K2-Instruct qua NVIDIA NIM b·∫±ng API OpenAI-compatible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "\n",
    "        # return resp.choices[0].message[\"content\"].strip()   ==>  s·ª≠a t·ª´ gemini --> Kimi\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM ERROR] {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ---------------- Configuration dataclass (Nano-like) ----------------\n",
    "@dataclass\n",
    "class GraphRAGConfig:\n",
    "    working_dir: str = field(\n",
    "        default_factory=lambda: f\"./nano_graphrag_cache_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    )\n",
    "\n",
    "    # graph mode\n",
    "    enable_local: bool = True\n",
    "    enable_naive_rag: bool = False\n",
    "\n",
    "    # text chunking\n",
    "    tokenizer_type: str = \"tiktoken\"  # or 'huggingface' or 'char'\n",
    "    # tiktoken_model_name: str = \"gpt-4o\"\n",
    "    tiktoken_model_name: str = \"cl100k_base\"\n",
    "\n",
    "    huggingface_model_name: str = \"bert-base-uncased\"\n",
    "    chunk_token_size: int = 1000\n",
    "    chunk_overlap_token_size: int = 100\n",
    "    chunk_char_size: int = 1200\n",
    "    chunk_char_overlap: int = 100\n",
    "    chunk_func: Optional[Callable] = None  # can be set to a custom chunking function\n",
    "\n",
    "    # entity extraction\n",
    "    entity_extract_max_gleaning: int = 1\n",
    "    entity_summary_to_max_tokens: int = 500\n",
    "\n",
    "    # graph clustering\n",
    "    graph_cluster_algorithm: str = \"greedy\"  # 'leiden' or 'louvain' or 'none'\n",
    "    max_graph_cluster_size: int = 8\n",
    "    graph_cluster_seed: int = 0xDEADBEEF\n",
    "\n",
    "    # node embedding\n",
    "    node_embedding_algorithm: str = \"node2vec\"\n",
    "    node2vec_params: dict = field(\n",
    "        default_factory=lambda: {\n",
    "            \"dimensions\": 128,\n",
    "            \"num_walks\": 10,\n",
    "            \"walk_length\": 40,\n",
    "            \"window_size\": 3,\n",
    "            \"iterations\": 3,\n",
    "            \"random_seed\": 3,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # community reports\n",
    "    special_community_report_llm_kwargs: dict = field(\n",
    "        default_factory=lambda: {\"response_format\": {\"type\": \"json_object\"}}\n",
    "    )\n",
    "\n",
    "    # text embedding (batching)\n",
    "    embedding_batch_num: int = 32\n",
    "    embedding_func_max_async: int = 16\n",
    "    query_better_than_threshold: float = 0.2\n",
    "\n",
    "\n",
    "# ---------------- Token-based splitter (tiktoken) ----------------\n",
    "class TokenSplitter:\n",
    "    def __init__(self, cfg: GraphRAGConfig):\n",
    "        self.cfg = cfg\n",
    "        if tiktoken is None:\n",
    "            raise RuntimeError(\n",
    "                \"tiktoken not installed; install with `pip install tiktoken`\"\n",
    "            )\n",
    "\n",
    "        # üîí Stable tokenizer\n",
    "        try:\n",
    "            self.enc = tiktoken.encoding_for_model(cfg.tiktoken_model_name)\n",
    "        except KeyError:\n",
    "            logger.warning(\n",
    "                f\"tiktoken cannot map model '{cfg.tiktoken_model_name}'. \"\n",
    "                \"Falling back to cl100k_base.\"\n",
    "            )\n",
    "            self.enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "        self.chunk_size = cfg.chunk_token_size\n",
    "        self.overlap = cfg.chunk_overlap_token_size\n",
    "\n",
    "    def chunk(self, text: str) -> List[str]:\n",
    "        tokens = self.enc.encode(text)\n",
    "        chunks: List[str] = []\n",
    "        step = max(1, self.chunk_size - self.overlap)\n",
    "        for i in range(0, len(tokens), step):\n",
    "            sub = tokens[i:i + self.chunk_size]\n",
    "            if not sub:\n",
    "                continue\n",
    "            chunks.append(self.enc.decode(sub))\n",
    "        return chunks\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def split_long_context(text, max_chars=800):\n",
    "    \"\"\"\n",
    "    Chia 1 context d√†i th√†nh nhi·ªÅu ƒëo·∫°n nh·ªè (~800 k√Ω t·ª±),\n",
    "    ƒë·∫£m b·∫£o m·ªói ƒëo·∫°n v·∫´n l√† 1 ƒëo·∫°n vƒÉn logic.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for s in sentences:\n",
    "        if len(current) + len(s) <= max_chars:\n",
    "            current += \" \" + s\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = s\n",
    "\n",
    "    if current.strip():\n",
    "        chunks.append(current.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ---------------- Main SimpleGraphRAG (keeps original API) ----------------\n",
    "class SimpleGraphRAG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 1200,\n",
    "        chunk_overlap: int = 100,\n",
    "        faiss_index_factory: Optional[str] = None,\n",
    "        cfg: Optional[GraphRAGConfig] = None,\n",
    "    ):\n",
    "        # preserve original args\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.faiss_index_factory = faiss_index_factory\n",
    "\n",
    "        # config (nano-like)\n",
    "        self.cfg = cfg or GraphRAGConfig()\n",
    "        os.makedirs(self.cfg.working_dir, exist_ok=True)\n",
    "\n",
    "        # core components\n",
    "        self.dataset: List[str] = []\n",
    "        self.chunk_entities: List[Set[str]] = []\n",
    "        self.nlp = None\n",
    "        self.embedding_model = None\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.index: Optional[faiss.Index] = None\n",
    "        self.kg: Optional[nx.DiGraph] = None\n",
    "        self.node_embeddings: Optional[Dict[str, np.ndarray]] = None\n",
    "\n",
    "        # initialize\n",
    "        self._init_models()\n",
    "\n",
    "    def _init_models(self):\n",
    "        logger.info(\"Loading embedding model...\")\n",
    "        self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
    "\n",
    "        logger.info(\"Loading spaCy model...\")\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.nlp.max_length = 10_000_000\n",
    "        except OSError:\n",
    "            raise RuntimeError(\"Ch∆∞a c√≥ model spaCy. Ch·∫°y: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "    # ---------------- Load + Chunk ----------------\n",
    "    def load_json_and_concat(self, json_path: str, text_key: str = \"context\") -> str:\n",
    "        logger.info(f\"Loading JSON from {json_path} ...\")\n",
    "        try:\n",
    "            df = pd.read_json(json_path)\n",
    "            if text_key not in df.columns:\n",
    "                raise ValueError(f\"Key '{text_key}' not found. Columns: {df.columns.tolist()}\")\n",
    "            full_text = \" \".join(df[text_key].astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"L·ªói ƒë·ªçc JSON: {e}. D√πng fallback text.\")\n",
    "            full_text = (\n",
    "                \"Basal cell carcinoma (BCC) is the most common type of skin cancer. \"\n",
    "                \"It is frequently treated with surgery. Fair skin increases the risk.\"\n",
    "            )\n",
    "        return full_text\n",
    "\n",
    "    # import re\n",
    "\n",
    "    # def split_long_context(text, max_chars=1000):\n",
    "    #     sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    #     chunks = []\n",
    "    #     current = \"\"\n",
    "\n",
    "    #     for s in sentences:\n",
    "    #         if len(current) + len(s) < max_chars:\n",
    "    #             current += \" \" + s\n",
    "    #         else:\n",
    "    #             chunks.append(current.strip())\n",
    "    #             current = s\n",
    "\n",
    "    #     if current:\n",
    "    #         chunks.append(current.strip())\n",
    "    #     return chunks\n",
    "\n",
    "    def chunk_text(self, text: str, method: str = \"auto\"):\n",
    "        \"\"\"\n",
    "        method: 'auto' -> choose based on cfg.tokenizer_type\n",
    "                'token' -> tiktoken\n",
    "                'char' -> RecursiveCharacterTextSplitter\n",
    "        \"\"\"\n",
    "        logger.info(\"Chunking text ...\")\n",
    "        # use custom chunk func if provided\n",
    "        if self.cfg.chunk_func is not None:\n",
    "            logger.info(\"Using custom chunk_func from cfg\")\n",
    "            chunks = self.cfg.chunk_func(text)\n",
    "            self.dataset = [c[\"text\"] if isinstance(c, dict) and \"text\" in c else str(c) for c in chunks]\n",
    "            logger.info(f\"Created {len(self.dataset)} chunks (custom function)\")\n",
    "            return\n",
    "\n",
    "        mode = method\n",
    "        if method == \"auto\":\n",
    "            mode = \"token\" if self.cfg.tokenizer_type == \"tiktoken\" and tiktoken is not None else \"char\"\n",
    "\n",
    "        if mode == \"token\":\n",
    "            if tiktoken is None:\n",
    "                logger.warning(\"tiktoken not available; falling back to char splitter\")\n",
    "                self.dataset = char_splitter(text, self.cfg.chunk_char_size, self.cfg.chunk_char_overlap)\n",
    "            else:\n",
    "                splitter = TokenSplitter(self.cfg)\n",
    "                self.dataset = splitter.chunk(text)\n",
    "        else:\n",
    "            self.dataset = char_splitter(text, self.cfg.chunk_char_size, self.cfg.chunk_char_overlap)\n",
    "\n",
    "        logger.info(f\"Created {len(self.dataset)} chunks (mode={mode})\")\n",
    "\n",
    "    # ---------------- Embeddings + FAISS ----------------\n",
    "    def build_embeddings_and_index(self, normalize: bool = True, batch_size: Optional[int] = None):\n",
    "        if not self.dataset:\n",
    "            raise RuntimeError(\"Dataset empty. Run chunk_text() first.\")\n",
    "        batch_size = batch_size or self.cfg.embedding_batch_num\n",
    "        logger.info(\"Computing embeddings ...\")\n",
    "        emb_list = []\n",
    "        for i in range(0, len(self.dataset), batch_size):\n",
    "            batch = self.dataset[i:i + batch_size]\n",
    "            emb_batch = self.embedding_model.encode(batch, normalize_embeddings=normalize)\n",
    "            emb_list.append(emb_batch)\n",
    "        emb = np.vstack(emb_list).astype(\"float32\")\n",
    "        # extra safe-normalize\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        emb = emb / norms\n",
    "        self.embeddings = emb\n",
    "\n",
    "        d = emb.shape[1]\n",
    "        logger.info(f\"Creating FAISS index (dimension={d})\")\n",
    "        if self.faiss_index_factory:\n",
    "            try:\n",
    "                self.index = faiss.index_factory(d, self.faiss_index_factory)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"faiss.index_factory failed: {e}, falling back to IndexFlatIP\")\n",
    "                self.index = faiss.IndexFlatIP(d)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(d)\n",
    "        self.index.add(emb)\n",
    "        logger.info(\"FAISS index built\")\n",
    "\n",
    "        # cache embeddings\n",
    "        try:\n",
    "            np.save(os.path.join(self.cfg.working_dir, \"embeddings.npy\"), self.embeddings)\n",
    "            logger.info(f\"Saved embeddings to {self.cfg.working_dir}/embeddings.npy\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def build_kg(self, use_dependency: bool = True):\n",
    "        \"\"\"\n",
    "        Build a Knowledge Graph using chunk-based NER (safe for large datasets).\n",
    "        Instead of processing full_text (which can exceed spaCy limits),\n",
    "        we run spaCy only on each chunk.\n",
    "        \"\"\"\n",
    "        logger.info(\"Building Knowledge Graph (chunk-based NER)...\")\n",
    "\n",
    "        self.kg = nx.DiGraph()\n",
    "        entity_nodes = set()\n",
    "        self.chunk_entities = []\n",
    "\n",
    "        # --- NER + relation extraction per chunk ---\n",
    "        for chunk in self.dataset:\n",
    "            cdoc = self.nlp(chunk)\n",
    "\n",
    "            # entities in this chunk\n",
    "            # ents = [e.text.strip() for e in cdoc.ents if e.text.strip()]\n",
    "            # --- robust entity extraction (medical-safe) ---\n",
    "            ents = set()\n",
    "\n",
    "            # 1Ô∏è‚É£ spaCy NER (n·∫øu c√≥)\n",
    "            for ent in cdoc.ents:\n",
    "                ents.add(ent.text.strip())\n",
    "\n",
    "            # 2Ô∏è‚É£ fallback: noun chunks (QUAN TR·ªåNG)\n",
    "            for np in cdoc.noun_chunks:\n",
    "                text = np.text.strip().lower()\n",
    "                if len(text.split()) >= 2:          # b·ªè noun qu√° ng·∫Øn\n",
    "                    ents.add(text)\n",
    "\n",
    "            ents = list(ents)\n",
    "\n",
    "            ent_set = set(ents)\n",
    "            self.chunk_entities.append(ent_set)\n",
    "\n",
    "            # add entity nodes\n",
    "            for e in ent_set:\n",
    "                if e not in self.kg:\n",
    "                    # use entity label if available (PERSON, ORG, etc)\n",
    "                    label = None\n",
    "                    for ent in cdoc.ents:\n",
    "                        if ent.text.strip() == e:\n",
    "                            label = ent.label_\n",
    "                            break\n",
    "                    self.kg.add_node(e, label=label or \"UNKNOWN\")\n",
    "                entity_nodes.add(e)\n",
    "\n",
    "            # extract relations inside this chunk\n",
    "            for sent in cdoc.sents:\n",
    "                sent_ents = [e.text.strip() for e in sent.ents]\n",
    "\n",
    "                # --- dependency-based relation extraction ---\n",
    "                if use_dependency:\n",
    "                    subj = None\n",
    "                    obj = None\n",
    "                    verb = None\n",
    "                    for token in sent:\n",
    "                        if \"subj\" in token.dep_:\n",
    "                            subj = token.text\n",
    "                            verb = token.head.lemma_\n",
    "                            for child in token.head.children:\n",
    "                                if \"obj\" in child.dep_:\n",
    "                                    obj = child.text\n",
    "                                    if subj in entity_nodes and obj in entity_nodes:\n",
    "                                        self.kg.add_edge(subj, obj, relation=verb)\n",
    "\n",
    "                # --- fallback: co-occurrence graph ---\n",
    "                for i in range(len(sent_ents)):\n",
    "                    for j in range(i + 1, len(sent_ents)):\n",
    "                        a, b = sent_ents[i], sent_ents[j]\n",
    "                        if a != b and not self.kg.has_edge(a, b):\n",
    "                            self.kg.add_edge(a, b, relation=\"co_occurs_with\")\n",
    "\n",
    "        logger.info(f\"KG built with {self.kg.number_of_nodes()} nodes, {self.kg.number_of_edges()} edges\")\n",
    "\n",
    "\n",
    "    # ---------------- Graph clustering (optional) ----------------\n",
    "    def cluster_graph(self):\n",
    "        \n",
    "        \n",
    "        algo = self.cfg.graph_cluster_algorithm.lower()\n",
    "        if algo == \"none\":\n",
    "            logger.info(\"Graph clustering disabled (algorithm=none).\")\n",
    "            return None\n",
    "\n",
    "        # --- Leiden via igraph (optional) ---\n",
    "        try:\n",
    "            import igraph as ig\n",
    "            import leidenalg\n",
    "            if algo == \"leiden\":\n",
    "                logger.info(\"Running Leiden clustering via igraph + leidenalg.\")\n",
    "                g = ig.Graph.TupleList(self.kg.edges(), directed=True)\n",
    "                partition = leidenalg.find_partition(g, leidenalg.RBConfigurationVertexPartition)\n",
    "                clusters = [list(map(str, com)) for com in partition]\n",
    "                logger.info(f\"Leiden found {len(clusters)} clusters.\")\n",
    "                return clusters\n",
    "        except ImportError:\n",
    "            logger.warning(\"igraph or leidenalg not installed; skipping Leiden clustering.\")\n",
    "\n",
    "        # --- NetworkX community fallback ---\n",
    "        if nx_community is not None and algo in (\"louvain\", \"greedy\", \"labelprop\"):\n",
    "            logger.info(\"Running NetworkX community clustering (fallback).\")\n",
    "            communities = nx_community.greedy_modularity_communities(self.kg.to_undirected())\n",
    "            clusters = [list(c) for c in communities]\n",
    "            logger.info(f\"NetworkX found {len(clusters)} communities.\")\n",
    "            return clusters\n",
    "\n",
    "        logger.warning(\"No clustering library available or unsupported algorithm; skipping clustering.\")\n",
    "        return None\n",
    "\n",
    "    # ---------------- Node embeddings (node2vec) ----------------\n",
    "    def embed_graph_nodes(self):\n",
    "        if Node2Vec is None:\n",
    "            logger.warning(\"node2vec not installed; skipping node embedding.\")\n",
    "            return None\n",
    "        if self.kg is None or self.kg.number_of_nodes() == 0:\n",
    "            logger.warning(\"KG empty; run build_kg() first.\")\n",
    "            return None\n",
    "        params = self.cfg.node2vec_params\n",
    "        logger.info(\"Running Node2Vec to generate node embeddings...\")\n",
    "        # node2vec expects undirected or directed networkx graph\n",
    "        node2vec = Node2Vec(self.kg, dimensions=params.get(\"dimensions\", 128),\n",
    "                            walk_length=params.get(\"walk_length\", 40),\n",
    "                            num_walks=params.get(\"num_walks\", 10),\n",
    "                            workers=2)\n",
    "        model = node2vec.fit(window=params.get(\"window_size\", 3), min_count=1, batch_words=4)\n",
    "        # store embeddings in dict\n",
    "        self.node_embeddings = {node: model.wv[str(node)] for node in self.kg.nodes()}\n",
    "        logger.info(f\"Generated node embeddings for {len(self.node_embeddings)} nodes.\")\n",
    "        # optional cache\n",
    "        try:\n",
    "            np.save(os.path.join(self.cfg.working_dir, \"node_embeddings.npy\"), self.node_embeddings)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return self.node_embeddings\n",
    "\n",
    "    # ---------------- GraphRAG Query (hybrid) ----------------\n",
    "    def _semantic_search(self, query: str, top_k: int = 5):\n",
    "        q_emb = self.embedding_model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "        D, I = self.index.search(q_emb, top_k)\n",
    "        return [(int(i), float(D[0][idx])) for idx, i in enumerate(I[0])]\n",
    "\n",
    "    def _graph_search_scores(self, query: str):\n",
    "        q_doc = self.nlp(query)\n",
    "        q_entities = set([ent.text.strip() for ent in q_doc.ents if ent.text.strip()])\n",
    "        if len(q_entities) == 0:\n",
    "            return np.zeros(len(self.dataset), dtype=float)\n",
    "        scores = np.array([len(self.chunk_entities[i].intersection(q_entities)) for i in range(len(self.dataset))], dtype=float)\n",
    "        if scores.max() > 0:\n",
    "            scores = scores / (scores.max() + 1e-12)\n",
    "        return scores\n",
    "\n",
    "    def graphrag_query(self, query: str, top_k: int = 5, alpha: float = 0.8):\n",
    "        \"\"\"\n",
    "        Combine semantic + graph signals.\n",
    "        alpha: weight for semantic retrieval (0..1). final_score = alpha*sem + (1-alpha)*graph\n",
    "        \"\"\"\n",
    "        # semantic candidates\n",
    "        sem = self._semantic_search(query, top_k=top_k * 2)  # retrieve more to allow reranking\n",
    "        sem_idxs = [i for i, s in sem]\n",
    "        sem_scores = {i: s for (i, s) in sem}\n",
    "\n",
    "        # graph scores across dataset\n",
    "        graph_scores = self._graph_search_scores(query)\n",
    "\n",
    "        # compute combined score for sem candidates\n",
    "        combined = []\n",
    "        for i, sem_score in sem:\n",
    "            gscore = graph_scores[i] if i < len(graph_scores) else 0.0\n",
    "            score = alpha * sem_score + (1.0 - alpha) * gscore\n",
    "            combined.append((i, score))\n",
    "\n",
    "        # sort and pick top_k\n",
    "        combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        combined_idxs = [i for i, s in combined_sorted]\n",
    "        combined_chunks = [self.dataset[i] for i in combined_idxs]\n",
    "\n",
    "        # gather also top graph-only chunks (if graph points to others not in sem candidates)\n",
    "        top_graph_idx = np.argsort(graph_scores)[-top_k:][::-1]\n",
    "        graph_chunks = [self.dataset[i] for i in top_graph_idx if graph_scores[i] > 0 and i not in combined_idxs]\n",
    "\n",
    "        # KG facts extraction (neighbours of query entities)\n",
    "        kg_facts = []\n",
    "        q_doc = self.nlp(query)\n",
    "        q_entities = set([ent.text.strip() for ent in q_doc.ents if ent.text.strip()])\n",
    "        for qe in q_entities:\n",
    "            if self.kg is not None and qe in self.kg:\n",
    "                # successors & predecessors (limit to a few)\n",
    "                for nb in list(self.kg.successors(qe))[:3]:\n",
    "                    rel = self.kg.get_edge_data(qe, nb).get(\"relation\", \"related_to\")\n",
    "                    kg_facts.append(f\"{qe} {rel} {nb}.\")\n",
    "                for nb in list(self.kg.predecessors(qe))[:3]:\n",
    "                    rel = self.kg.get_edge_data(nb, qe).get(\"relation\", \"related_to\")\n",
    "                    kg_facts.append(f\"{nb} {rel} {qe}.\")\n",
    "\n",
    "        combined_context = \" \".join(combined_chunks + graph_chunks + kg_facts)\n",
    "        # combined_context = \" \".join(combined_chunks[:3] + kg_facts[:4])\n",
    "            \n",
    "        return {\"query\": query, \"context\": combined_context, \"sem_idxs\": combined_idxs}\n",
    "\n",
    "    # ---------------- LLM Answer ----------------\n",
    "    def answer_question_llm(self, query: str, llm_model: str = \"moonshotai/kimi-k2-instruct-0905\", top_k: int = 5, alpha: float = 0.8):\n",
    "        result = self.graphrag_query(query, top_k=top_k, alpha=alpha)\n",
    "        context_text = result[\"context\"]\n",
    "        prompt = f\"\"\"\n",
    "You are a high-accuracy QA assistant. \n",
    "\n",
    "You MUST answer using ONLY the provided context.\n",
    "\n",
    "Your output must follow these rules:\n",
    "\n",
    "-------------------------------------------------\n",
    "DETECTION RULES (auto-detect the question type):\n",
    "- If the question asks for a specific name, place, person, object, event,\n",
    "  ‚Üí classify as: FACT RETRIEVAL.\n",
    "- If the question asks ‚Äúwhy‚Äù, ‚Äúhow‚Äù, ‚Äúwhat happened‚Äù, ‚Äúwhat was the relationship‚Äù,\n",
    "  or requires combining multiple clues,\n",
    "  ‚Üí classify as: COMPLEX REASONING.\n",
    "-------------------------------------------------\n",
    "\n",
    "ANSWER STYLE RULES:\n",
    "1. FACT RETRIEVAL:\n",
    "   - Answer in **3‚Äì12 words**.\n",
    "   - Must be short, factual, and contain ONLY the requested entity.\n",
    "   - Do NOT add explanations or context.\n",
    "\n",
    "2. COMPLEX REASONING:\n",
    "   - Answer in 1‚Äì2 concise sentences (12‚Äì25 words total).\n",
    "   - Combine only the essential evidence from the context.\n",
    "   - No paraphrasing of long passages; no extra narrative.\n",
    "   - The answer must be strictly grounded in the provided context.\n",
    "\n",
    "3. If the question_type is \"Contextual Summarize\":\n",
    "   - Produce 2‚Äì4 sentences.\n",
    "   - Length must be 80‚Äì200 words.\n",
    "   - Provide a smooth, coherent summary combining *all* major points from context.\n",
    "   - Use descriptive but concise language.\n",
    "   - Do NOT shorten aggressively.\n",
    "   - Do NOT reduce to a single-sentence summary.\n",
    "   - This rule overrides any sentence-limit rules.\n",
    "\n",
    "4. If the question_type is \"Creative Generation\":\n",
    "   - Follow the exact requested format (diary entry, newspaper article, retelling, etc.).\n",
    "   - Stay grounded in the given evidence; do not introduce new events or characters.\n",
    "   - 3‚Äì6 sentences depending on style, expressive but controlled.\n",
    "\n",
    "5. GENERAL RULES:\n",
    "    - Do NOT add information not present in the context.\n",
    "    - If context is incomplete, give the best-supported answer.\n",
    "    - If no relevant information exists, reply EXACTLY:\n",
    "    no relevant information found\n",
    "\n",
    "-------------------------------------------------\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "        # fix: ch·∫°y async function trong sync\n",
    "        # import asyncio\n",
    "        # answer = asyncio.run(llm_generate(prompt, model=llm_model))\n",
    "        answer = llm_generate(prompt, model=llm_model)\n",
    "\n",
    "        return answer, result\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- Evaluation ----------------\n",
    "    def evaluate_answer(self, pred: str, truth: str) -> float:\n",
    "        scorer = Rouge()\n",
    "        scores = scorer.get_scores(pred, truth)\n",
    "        return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "    # ---------------- Cache helpers ----------------\n",
    "    def save_index(self, fname: str = \"faiss.index\"):\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save.\")\n",
    "        path = os.path.join(self.cfg.working_dir, fname)\n",
    "        faiss.write_index(self.index, path)\n",
    "        logger.info(f\"FAISS index written to {path}\")\n",
    "\n",
    "    def load_index(self, fname: str = \"faiss.index\"):\n",
    "        path = os.path.join(self.cfg.working_dir, fname)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(path)\n",
    "        self.index = faiss.read_index(path)\n",
    "        logger.info(f\"FAISS index loaded from {path}\")\n",
    "\n",
    "    def save_embeddings(self, fname: str = \"embeddings.npy\"):\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No embeddings to save.\")\n",
    "        np.save(os.path.join(self.cfg.working_dir, fname), self.embeddings)\n",
    "        logger.info(\"Embeddings saved.\")\n",
    "\n",
    "    def load_embeddings(self, fname: str = \"embeddings.npy\"):\n",
    "        path = os.path.join(self.cfg.working_dir, fname)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(path)\n",
    "        self.embeddings = np.load(path)\n",
    "        logger.info(\"Embeddings loaded.\")\n",
    "\n",
    "    # ---------------- Convenience run -> inference on JSON questions ----------------\n",
    "def run_inference_on_medical_json(rag, json_path, num_questions=10,question_type_filter=\"Fact Retrieval\"):\n",
    "    \"\"\"\n",
    "    Ch·∫°y LLM tr√™n c√°c c√¢u h·ªèi JSON ƒë∆∞·ª£c l·ªçc theo question_type.\n",
    "    M·∫∑c ƒë·ªãnh ch·∫°y c√°c c√¢u 'Fact Retrieval'.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # L·ªçc c√°c c√¢u h·ªèi theo lo·∫°i y√™u c·∫ßu\n",
    "    # L·ªçc c√°c c√¢u h·ªèi theo lo·∫°i y√™u c·∫ßu\n",
    "    filtered = [\n",
    "        item for item in data\n",
    "        if item.get(\"question_type\", \"\").lower() == question_type_filter.lower()\n",
    "    ]\n",
    "\n",
    "    if not filtered:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi lo·∫°i '{question_type_filter}' trong file: {json_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîπ Found {len(filtered)} questions of type '{question_type_filter}'\")\n",
    "    print(f\"‚û° Running inference on first {num_questions}...\\n\")\n",
    "\n",
    "    # print(f\"üîπ Running inference on {num_questions} questions from {json_path}\\n\")\n",
    "\n",
    "    # ‚ùó USE filtered list, NOT full dataset\n",
    "    for i, item in enumerate(filtered[:num_questions]):\n",
    "        question = item.get(\"question\", \"\") or item.get(\"q\", \"\")\n",
    "        ground_truth = item.get(\"answer\", \"\") or item.get(\"a\", \"\")\n",
    "        # question_type = item.get(\"question_type\", \"\")\n",
    "        # g·ªçi LLM\n",
    "        ans, _ = rag.answer_question_llm(question)\n",
    "\n",
    "        # x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi\n",
    "        sentences = [s.strip() for s in ans.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "        short_ans = \". \".join(sentences[:5]).strip()\n",
    "        if short_ans and not short_ans.endswith(\".\"):\n",
    "            short_ans += \".\"\n",
    "\n",
    "        # in k·∫øt qu·∫£\n",
    "        print(f\"=== Q{i+1}: {question}\")\n",
    "        print(f\"Kimi K2 Answer:\\n{short_ans}\\n\")\n",
    "        print(f\"Ground Truth:\\n{ground_truth}\\n\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "def load_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # create rag with default config (you can pass a GraphRAGConfig instance to override)\n",
    "    rag = SimpleGraphRAG(embedding_model_name=\"all-MiniLM-L6-v2\", chunk_size=1200, chunk_overlap=100)\n",
    "\n",
    "    \n",
    "    # 1) load corpus# Paths (change to your local dataset)\n",
    "    # corpus_path = \"/KGC/SAT/aligner/data/FB15k-237N/id2text.txt\"\n",
    "    corpus_path = \"SAT/aligner/data/FB15k-237N/id3text.txt\"\n",
    "\n",
    "\n",
    "    # questions_path = \"/Users/duyanhle1501/Duy Anh Le Code/GraphRAG-Benchmark/Datasets/Questions/medical_questions.json\"\n",
    "\n",
    "    # text = rag.load_json_and_concat(corpus_path)\n",
    "    text = load_txt(corpus_path)\n",
    "\n",
    "    rag.dataset = split_long_context(text, max_chars=800)\n",
    "\n",
    "    # 2) chunk (uses config tokenizer; default is tiktoken if available)\n",
    "    # rag.chunk_text(text, method=\"auto\")\n",
    "\n",
    "    # 3) embedding + index\n",
    "    rag.build_embeddings_and_index()\n",
    "\n",
    "    # 4) build a proper KG (NER + simple dependency relations)\n",
    "    rag.build_kg(use_dependency=True)\n",
    "\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    os.makedirs(\"sat_data\", exist_ok=True)\n",
    "\n",
    "    with open(\"sat_data/kg.pkl\", \"wb\") as f:\n",
    "        pickle.dump(rag.kg, f)\n",
    "\n",
    "    print(\"‚úÖ Saved KG to sat_data/kg.pkl\")\n",
    "    print(\"Nodes:\", rag.kg.number_of_nodes())\n",
    "    print(\"Edges:\", rag.kg.number_of_edges())\n",
    "\n",
    "\n",
    "    # 5) optional: cluster graph & embed nodes\n",
    "    # clusters = rag.cluster_graph()\n",
    "    # node_embs = rag.embed_graph_nodes()\n",
    "\n",
    "    # 6) run inference on first 5 questions and print top 5-sentence summary\n",
    "    # run_inference_on_medical_json(rag, questions_path, num_questions=10, question_type_filter=\"Fact Retrieval\")\n",
    "\n",
    "    # ====== SAVE CONTEXTS FOR PART B ======\n",
    "    # import json\n",
    "\n",
    "    # contexts = []\n",
    "\n",
    "    # with open(questions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    #     data = json.load(f)\n",
    "\n",
    "    # filtered = [\n",
    "    #     item for item in data\n",
    "    #     if item.get(\"question_type\", \"\").lower() == \"fact retrieval\"\n",
    "    # ]\n",
    "\n",
    "    # for item in filtered[:10]:\n",
    "    #     q = item[\"question\"]\n",
    "    #     r = rag.graphrag_query(q)\n",
    "    #     contexts.append({\n",
    "    #         \"question\": q,\n",
    "    #         \"context\": r[\"context\"]\n",
    "    #     })\n",
    "\n",
    "    # with open(\"contexts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #     json.dump(contexts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # print(\"‚úÖ contexts.json saved (Part A done)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SimpleGraphRAG:Loading JSON from /Users/duyanhle1501/Duy Anh Le Code/GraphRAG-Benchmark/Datasets/Corpus/medical.json ...\n",
      "WARNING:SimpleGraphRAG:L·ªói ƒë·ªçc JSON: If using all scalar values, you must pass an index. D√πng fallback text.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BCC', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "text = rag.load_json_and_concat(corpus_path)\n",
    "doc = rag.nlp(text[:1000])\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th√™m QWEN N√à\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (2.9.1)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2025.12.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2026.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/miniconda3/envs/graphrag-qwen/lib/python3.10/site-packages (from transformers->peft) (0.22.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipykernel\n",
    "# !python -m ipykernel install --user --name kgc-venv --display-name \"KGC (.venv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simple_graphrag.py (upgraded -> v3.0)\n",
    "# import os\n",
    "# import json\n",
    "# import logging\n",
    "# from typing import List, Set, Tuple, Dict, Any, Optional, Callable, Union\n",
    "# from dataclasses import dataclass, field\n",
    "# from datetime import datetime\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import faiss\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# import spacy\n",
    "# import networkx as nx\n",
    "# from rouge import Rouge\n",
    "# import os\n",
    "\n",
    "# # ---------- Local Qwen (SAT LoRA) ----------\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel\n",
    "\n",
    "# QWEN_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# SAT_LORA_PATH = \"sat_lora_model\"\n",
    "\n",
    "# def load_qwen_sat():\n",
    "#     print(\"üîπ Loading Qwen + SAT LoRA (CPU only, safe)\")\n",
    "\n",
    "#     device = \"cpu\"\n",
    "#     dtype = torch.float32   # B·∫ÆT BU·ªòC\n",
    "\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME)\n",
    "\n",
    "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#         QWEN_MODEL_NAME,\n",
    "#         dtype=dtype,\n",
    "#         device_map=None,\n",
    "#         low_cpu_mem_usage=True\n",
    "#     )\n",
    "\n",
    "#     model = PeftModel.from_pretrained(base_model, SAT_LORA_PATH)\n",
    "#     model.eval()\n",
    "\n",
    "#     return tokenizer, model, device\n",
    "\n",
    "# # QWEN_TOKENIZER, QWEN_MODEL, QWEN_DEVICE = load_qwen_sat()\n",
    "# # QWEN_TOKENIZER = None\n",
    "# # QWEN_MODEL = None\n",
    "# # QWEN_DEVICE = None\n",
    "\n",
    "\n",
    "\n",
    "# def qwen_reasoning(query: str, context: str, max_tokens: int = 128) -> str:\n",
    "#     global QWEN_TOKENIZER, QWEN_MODEL, QWEN_DEVICE\n",
    "\n",
    "#     if QWEN_MODEL is None:\n",
    "#         QWEN_TOKENIZER, QWEN_MODEL, QWEN_DEVICE = load_qwen_sat()\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# You are a reasoning assistant trained on SAT-style logic.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {query}\n",
    "\n",
    "# Reasoning:\n",
    "# \"\"\"\n",
    "\n",
    "#     inputs = QWEN_TOKENIZER(prompt, return_tensors=\"pt\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = QWEN_MODEL.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=max_tokens,\n",
    "#             do_sample=False\n",
    "#         )\n",
    "\n",
    "#     text = QWEN_TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return text.split(\"Reasoning:\")[-1].strip()\n",
    "\n",
    "# # ==============================\n",
    "# # üîπ Kimi K2 (Remote via NVIDIA NIM)\n",
    "# # ==============================\n",
    "\n",
    "# os.environ[\"NVAPI_KEY\"] = \"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\"\n",
    "# # 3) Create a client wrapper\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(\n",
    "#     base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "#     api_key=os.getenv(\"NVAPI_KEY\")\n",
    "# )\n",
    "\n",
    "# # Optional / extra libs (used if installed)\n",
    "# try:\n",
    "#     import tiktoken\n",
    "# except Exception:\n",
    "#     tiktoken = None\n",
    "\n",
    "# try:\n",
    "#     from node2vec import Node2Vec\n",
    "# except Exception:\n",
    "#     Node2Vec = None\n",
    "\n",
    "# # Optional clustering libs\n",
    "# try:\n",
    "#     import igraph as ig\n",
    "#     import leidenalg\n",
    "# except Exception:\n",
    "#     ig = None\n",
    "#     leidenalg = None\n",
    "\n",
    "# # try:\n",
    "# from networkx.algorithms import community as nx_community\n",
    "# # except Exception:\n",
    "# #     nx_community = None\n",
    "\n",
    "# # ---------- Logging ----------\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(\"SimpleGraphRAG\")\n",
    "\n",
    "\n",
    "\n",
    "# # ---------- Utility LLM (Kimi K2) ----------\n",
    "# # async\n",
    "# def llm_generate(prompt: str,\n",
    "#                        model: str = \"moonshotai/kimi-k2-instruct-0905\",\n",
    "#                        temperature: float = 0.2,\n",
    "#                        max_tokens: int = 1024):\n",
    "#     \"\"\"\n",
    "#     G·ªçi Kimi-K2-Instruct qua NVIDIA NIM b·∫±ng API OpenAI-compatible.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         resp = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#             temperature=temperature,\n",
    "#             top_p=0.9,\n",
    "#             max_tokens=max_tokens\n",
    "#         )\n",
    "\n",
    "#         # return resp.choices[0].message[\"content\"].strip()   ==>  s·ª≠a t·ª´ gemini --> Kimi\n",
    "#         return resp.choices[0].message.content.strip()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"[LLM ERROR] {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# # ---------------- Configuration dataclass (Nano-like) ----------------\n",
    "# @dataclass\n",
    "# class GraphRAGConfig:\n",
    "#     working_dir: str = field(\n",
    "#         default_factory=lambda: f\"./nano_graphrag_cache_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "#     )\n",
    "\n",
    "#     # graph mode\n",
    "#     enable_local: bool = True\n",
    "#     enable_naive_rag: bool = False\n",
    "\n",
    "#     # text chunking\n",
    "#     tokenizer_type: str = \"tiktoken\"  # or 'huggingface' or 'char'\n",
    "#     # tiktoken_model_name: str = \"gpt-4o\"\n",
    "#     tiktoken_model_name: str = \"cl100k_base\"\n",
    "\n",
    "#     huggingface_model_name: str = \"bert-base-uncased\"\n",
    "#     chunk_token_size: int = 1000\n",
    "#     chunk_overlap_token_size: int = 100\n",
    "#     chunk_char_size: int = 1200\n",
    "#     chunk_char_overlap: int = 100\n",
    "#     chunk_func: Optional[Callable] = None  # can be set to a custom chunking function\n",
    "\n",
    "#     # entity extraction\n",
    "#     entity_extract_max_gleaning: int = 1\n",
    "#     entity_summary_to_max_tokens: int = 500\n",
    "\n",
    "#     # graph clustering\n",
    "#     graph_cluster_algorithm: str = \"greedy\"  # 'leiden' or 'louvain' or 'none'\n",
    "#     max_graph_cluster_size: int = 8\n",
    "#     graph_cluster_seed: int = 0xDEADBEEF\n",
    "\n",
    "#     # node embedding\n",
    "#     node_embedding_algorithm: str = \"node2vec\"\n",
    "#     node2vec_params: dict = field(\n",
    "#         default_factory=lambda: {\n",
    "#             \"dimensions\": 128,\n",
    "#             \"num_walks\": 10,\n",
    "#             \"walk_length\": 40,\n",
    "#             \"window_size\": 3,\n",
    "#             \"iterations\": 3,\n",
    "#             \"random_seed\": 3,\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # community reports\n",
    "#     special_community_report_llm_kwargs: dict = field(\n",
    "#         default_factory=lambda: {\"response_format\": {\"type\": \"json_object\"}}\n",
    "#     )\n",
    "\n",
    "#     # text embedding (batching)\n",
    "#     embedding_batch_num: int = 32\n",
    "#     embedding_func_max_async: int = 16\n",
    "#     query_better_than_threshold: float = 0.2\n",
    "\n",
    "\n",
    "# # ---------------- Token-based splitter (tiktoken) ----------------\n",
    "# class TokenSplitter:\n",
    "#     def __init__(self, cfg: GraphRAGConfig):\n",
    "#         self.cfg = cfg\n",
    "#         if tiktoken is None:\n",
    "#             raise RuntimeError(\n",
    "#                 \"tiktoken not installed; install with `pip install tiktoken`\"\n",
    "#             )\n",
    "\n",
    "#         # üîí Stable tokenizer\n",
    "#         try:\n",
    "#             self.enc = tiktoken.encoding_for_model(cfg.tiktoken_model_name)\n",
    "#         except KeyError:\n",
    "#             logger.warning(\n",
    "#                 f\"tiktoken cannot map model '{cfg.tiktoken_model_name}'. \"\n",
    "#                 \"Falling back to cl100k_base.\"\n",
    "#             )\n",
    "#             self.enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "#         self.chunk_size = cfg.chunk_token_size\n",
    "#         self.overlap = cfg.chunk_overlap_token_size\n",
    "\n",
    "#     def chunk(self, text: str) -> List[str]:\n",
    "#         tokens = self.enc.encode(text)\n",
    "#         chunks: List[str] = []\n",
    "#         step = max(1, self.chunk_size - self.overlap)\n",
    "#         for i in range(0, len(tokens), step):\n",
    "#             sub = tokens[i:i + self.chunk_size]\n",
    "#             if not sub:\n",
    "#                 continue\n",
    "#             chunks.append(self.enc.decode(sub))\n",
    "#         return chunks\n",
    "    \n",
    "# def char_splitter(text: str, size: int, overlap: int) -> List[str]:\n",
    "#         splitter = RecursiveCharacterTextSplitter(\n",
    "#             chunk_size=size,\n",
    "#             chunk_overlap=overlap,\n",
    "#             separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "#         )\n",
    "#         return [d.page_content for d in splitter.create_documents([text])]\n",
    "\n",
    "# # ==============================\n",
    "# # üîπ SimpleGraphRAG\n",
    "# # ==============================\n",
    "# class SimpleGraphRAG:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         embedding_model_name: str = \"all-MiniLM-L6-v2\",\n",
    "#         chunk_size: int = 1200,\n",
    "#         chunk_overlap: int = 100,\n",
    "#         faiss_index_factory: Optional[str] = None,\n",
    "#         cfg: Optional[GraphRAGConfig] = None,\n",
    "#     ):\n",
    "#         # preserve original args\n",
    "#         self.embedding_model_name = embedding_model_name\n",
    "#         self.chunk_size = chunk_size\n",
    "#         self.chunk_overlap = chunk_overlap\n",
    "#         self.faiss_index_factory = faiss_index_factory\n",
    "\n",
    "#         # config (nano-like)\n",
    "#         self.cfg = cfg or GraphRAGConfig()\n",
    "#         os.makedirs(self.cfg.working_dir, exist_ok=True)\n",
    "\n",
    "#         # core components\n",
    "#         self.dataset: List[str] = []\n",
    "#         self.chunk_entities: List[Set[str]] = []\n",
    "#         self.nlp = None\n",
    "#         self.embedding_model = None\n",
    "#         self.embeddings: Optional[np.ndarray] = None\n",
    "#         self.index: Optional[faiss.Index] = None\n",
    "#         self.kg: Optional[nx.DiGraph] = None\n",
    "#         self.node_embeddings: Optional[Dict[str, np.ndarray]] = None\n",
    "\n",
    "#         # initialize\n",
    "#         self._init_models()\n",
    "\n",
    "#     def _init_models(self):\n",
    "#         logger.info(\"Loading embedding model...\")\n",
    "#         self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
    "\n",
    "#         logger.info(\"Loading spaCy model...\")\n",
    "#         try:\n",
    "#             self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "#             self.nlp.max_length = 10_000_000\n",
    "#         except OSError:\n",
    "#             raise RuntimeError(\"Ch∆∞a c√≥ model spaCy. Ch·∫°y: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "#     # ---------------- Load + Chunk ----------------\n",
    "#     def load_json_and_concat(self, json_path: str, text_key: str = \"context\") -> str:\n",
    "#         logger.info(f\"Loading JSON from {json_path} ...\")\n",
    "#         try:\n",
    "#             df = pd.read_json(json_path)\n",
    "#             if text_key not in df.columns:\n",
    "#                 raise ValueError(f\"Key '{text_key}' not found. Columns: {df.columns.tolist()}\")\n",
    "#             full_text = \" \".join(df[text_key].astype(str).tolist())\n",
    "#         except Exception as e:\n",
    "#             logger.warning(f\"L·ªói ƒë·ªçc JSON: {e}. D√πng fallback text.\")\n",
    "#             full_text = (\n",
    "#                 \"Basal cell carcinoma (BCC) is the most common type of skin cancer. \"\n",
    "#                 \"It is frequently treated with surgery. Fair skin increases the risk.\"\n",
    "#             )\n",
    "#         return full_text\n",
    "\n",
    "#     def chunk_text(self, text: str, method: str = \"auto\"):\n",
    "#         \"\"\"\n",
    "#         method: 'auto' -> choose based on cfg.tokenizer_type\n",
    "#                 'token' -> tiktoken\n",
    "#                 'char' -> RecursiveCharacterTextSplitter\n",
    "#         \"\"\"\n",
    "#         logger.info(\"Chunking text ...\")\n",
    "#         # use custom chunk func if provided\n",
    "#         if self.cfg.chunk_func is not None:\n",
    "#             logger.info(\"Using custom chunk_func from cfg\")\n",
    "#             chunks = self.cfg.chunk_func(text)\n",
    "#             self.dataset = [c[\"text\"] if isinstance(c, dict) and \"text\" in c else str(c) for c in chunks]\n",
    "#             logger.info(f\"Created {len(self.dataset)} chunks (custom function)\")\n",
    "#             return\n",
    "\n",
    "#         mode = method\n",
    "#         if method == \"auto\":\n",
    "#             mode = \"token\" if self.cfg.tokenizer_type == \"tiktoken\" and tiktoken is not None else \"char\"\n",
    "\n",
    "#         if mode == \"token\":\n",
    "#             if tiktoken is None:\n",
    "#                 logger.warning(\"tiktoken not available; falling back to char splitter\")\n",
    "#                 self.dataset = char_splitter(text, self.cfg.chunk_char_size, self.cfg.chunk_char_overlap)\n",
    "#             else:\n",
    "#                 splitter = TokenSplitter(self.cfg)\n",
    "#                 self.dataset = splitter.chunk(text)\n",
    "#         else:\n",
    "#             self.dataset = char_splitter(text, self.cfg.chunk_char_size, self.cfg.chunk_char_overlap)\n",
    "\n",
    "#         logger.info(f\"Created {len(self.dataset)} chunks (mode={mode})\")\n",
    "\n",
    "#     # ---------------- Embeddings + FAISS ----------------\n",
    "#     def build_embeddings_and_index(self, normalize: bool = True, batch_size: Optional[int] = None):\n",
    "#         if not self.dataset:\n",
    "#             raise RuntimeError(\"Dataset empty. Run chunk_text() first.\")\n",
    "#         batch_size = batch_size or self.cfg.embedding_batch_num\n",
    "#         logger.info(\"Computing embeddings ...\")\n",
    "#         emb_list = []\n",
    "#         for i in range(0, len(self.dataset), batch_size):\n",
    "#             batch = self.dataset[i:i + batch_size]\n",
    "#             emb_batch = self.embedding_model.encode(batch, normalize_embeddings=normalize)\n",
    "#             emb_list.append(emb_batch)\n",
    "#         emb = np.vstack(emb_list).astype(\"float32\")\n",
    "#         # extra safe-normalize\n",
    "#         norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "#         norms[norms == 0.0] = 1.0\n",
    "#         emb = emb / norms\n",
    "#         self.embeddings = emb\n",
    "\n",
    "#         d = emb.shape[1]\n",
    "#         logger.info(f\"Creating FAISS index (dimension={d})\")\n",
    "#         if self.faiss_index_factory:\n",
    "#             try:\n",
    "#                 self.index = faiss.index_factory(d, self.faiss_index_factory)\n",
    "#             except Exception as e:\n",
    "#                 logger.warning(f\"faiss.index_factory failed: {e}, falling back to IndexFlatIP\")\n",
    "#                 self.index = faiss.IndexFlatIP(d)\n",
    "#         else:\n",
    "#             self.index = faiss.IndexFlatIP(d)\n",
    "#         self.index.add(emb)\n",
    "#         logger.info(\"FAISS index built\")\n",
    "\n",
    "#         # cache embeddings\n",
    "#         try:\n",
    "#             np.save(os.path.join(self.cfg.working_dir, \"embeddings.npy\"), self.embeddings)\n",
    "#             logger.info(f\"Saved embeddings to {self.cfg.working_dir}/embeddings.npy\")\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "    \n",
    "#     def build_kg(self, full_text: str, use_dependency: bool = True):\n",
    "#         \"\"\"\n",
    "#         Build a Knowledge Graph using chunk-based NER (safe for large datasets).\n",
    "#         Instead of processing full_text (which can exceed spaCy limits),\n",
    "#         we run spaCy only on each chunk.\n",
    "#         \"\"\"\n",
    "#         logger.info(\"Building Knowledge Graph (chunk-based NER)...\")\n",
    "\n",
    "#         self.kg = nx.DiGraph()\n",
    "#         entity_nodes = set()\n",
    "#         self.chunk_entities = []\n",
    "\n",
    "#         # --- NER + relation extraction per chunk ---\n",
    "#         for chunk in self.dataset:\n",
    "#             cdoc = self.nlp(chunk)\n",
    "\n",
    "#             # entities in this chunk\n",
    "#             ents = [e.text.strip() for e in cdoc.ents if e.text.strip()]\n",
    "#             ent_set = set(ents)\n",
    "#             self.chunk_entities.append(ent_set)\n",
    "\n",
    "#             # add entity nodes\n",
    "#             for e in ent_set:\n",
    "#                 if e not in self.kg:\n",
    "#                     # use entity label if available (PERSON, ORG, etc)\n",
    "#                     label = None\n",
    "#                     for ent in cdoc.ents:\n",
    "#                         if ent.text.strip() == e:\n",
    "#                             label = ent.label_\n",
    "#                             break\n",
    "#                     self.kg.add_node(e, label=label or \"UNKNOWN\")\n",
    "#                 entity_nodes.add(e)\n",
    "\n",
    "#             # extract relations inside this chunk\n",
    "#             for sent in cdoc.sents:\n",
    "#                 sent_ents = [e.text.strip() for e in sent.ents]\n",
    "\n",
    "#                 # --- dependency-based relation extraction ---\n",
    "#                 if use_dependency:\n",
    "#                     subj = None\n",
    "#                     obj = None\n",
    "#                     verb = None\n",
    "#                     for token in sent:\n",
    "#                         if \"subj\" in token.dep_:\n",
    "#                             subj = token.text\n",
    "#                             verb = token.head.lemma_\n",
    "#                             for child in token.head.children:\n",
    "#                                 if \"obj\" in child.dep_:\n",
    "#                                     obj = child.text\n",
    "#                                     if subj in entity_nodes and obj in entity_nodes:\n",
    "#                                         self.kg.add_edge(subj, obj, relation=verb)\n",
    "\n",
    "#                 # --- fallback: co-occurrence graph ---\n",
    "#                 for i in range(len(sent_ents)):\n",
    "#                     for j in range(i + 1, len(sent_ents)):\n",
    "#                         a, b = sent_ents[i], sent_ents[j]\n",
    "#                         if a != b and not self.kg.has_edge(a, b):\n",
    "#                             self.kg.add_edge(a, b, relation=\"co_occurs_with\")\n",
    "\n",
    "#         logger.info(f\"KG built with {self.kg.number_of_nodes()} nodes, {self.kg.number_of_edges()} edges\")\n",
    "\n",
    "\n",
    "#     # ---------------- Graph clustering (optional) ----------------\n",
    "#     def cluster_graph(self):\n",
    "        \n",
    "        \n",
    "#         algo = self.cfg.graph_cluster_algorithm.lower()\n",
    "#         if algo == \"none\":\n",
    "#             logger.info(\"Graph clustering disabled (algorithm=none).\")\n",
    "#             return None\n",
    "\n",
    "#         # --- Leiden via igraph (optional) ---\n",
    "#         try:\n",
    "#             import igraph as ig\n",
    "#             import leidenalg\n",
    "#             if algo == \"leiden\":\n",
    "#                 logger.info(\"Running Leiden clustering via igraph + leidenalg.\")\n",
    "#                 g = ig.Graph.TupleList(self.kg.edges(), directed=True)\n",
    "#                 partition = leidenalg.find_partition(g, leidenalg.RBConfigurationVertexPartition)\n",
    "#                 clusters = [list(map(str, com)) for com in partition]\n",
    "#                 logger.info(f\"Leiden found {len(clusters)} clusters.\")\n",
    "#                 return clusters\n",
    "#         except ImportError:\n",
    "#             logger.warning(\"igraph or leidenalg not installed; skipping Leiden clustering.\")\n",
    "\n",
    "#         # --- NetworkX community fallback ---\n",
    "#         if nx_community is not None and algo in (\"louvain\", \"greedy\", \"labelprop\"):\n",
    "#             logger.info(\"Running NetworkX community clustering (fallback).\")\n",
    "#             communities = nx_community.greedy_modularity_communities(self.kg.to_undirected())\n",
    "#             clusters = [list(c) for c in communities]\n",
    "#             logger.info(f\"NetworkX found {len(clusters)} communities.\")\n",
    "#             return clusters\n",
    "\n",
    "#         logger.warning(\"No clustering library available or unsupported algorithm; skipping clustering.\")\n",
    "#         return None\n",
    "\n",
    "#     # ---------------- Node embeddings (node2vec) ----------------\n",
    "#     def embed_graph_nodes(self):\n",
    "#         if Node2Vec is None:\n",
    "#             logger.warning(\"node2vec not installed; skipping node embedding.\")\n",
    "#             return None\n",
    "#         if self.kg is None or self.kg.number_of_nodes() == 0:\n",
    "#             logger.warning(\"KG empty; run build_kg() first.\")\n",
    "#             return None\n",
    "#         params = self.cfg.node2vec_params\n",
    "#         logger.info(\"Running Node2Vec to generate node embeddings...\")\n",
    "#         # node2vec expects undirected or directed networkx graph\n",
    "#         node2vec = Node2Vec(self.kg, dimensions=params.get(\"dimensions\", 128),\n",
    "#                             walk_length=params.get(\"walk_length\", 40),\n",
    "#                             num_walks=params.get(\"num_walks\", 10),\n",
    "#                             workers=2)\n",
    "#         model = node2vec.fit(window=params.get(\"window_size\", 3), min_count=1, batch_words=4)\n",
    "#         # store embeddings in dict\n",
    "#         self.node_embeddings = {node: model.wv[str(node)] for node in self.kg.nodes()}\n",
    "#         logger.info(f\"Generated node embeddings for {len(self.node_embeddings)} nodes.\")\n",
    "#         # optional cache\n",
    "#         try:\n",
    "#             np.save(os.path.join(self.cfg.working_dir, \"node_embeddings.npy\"), self.node_embeddings)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#         return self.node_embeddings\n",
    "\n",
    "#     # ---------------- GraphRAG Query (hybrid) ----------------\n",
    "#     def _semantic_search(self, query: str, top_k: int = 5):\n",
    "#         q_emb = self.embedding_model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "#         D, I = self.index.search(q_emb, top_k)\n",
    "#         return [(int(i), float(D[0][idx])) for idx, i in enumerate(I[0])]\n",
    "\n",
    "#     def _graph_search_scores(self, query: str):\n",
    "#         q_doc = self.nlp(query)\n",
    "#         q_entities = set([ent.text.strip() for ent in q_doc.ents if ent.text.strip()])\n",
    "#         if len(q_entities) == 0:\n",
    "#             return np.zeros(len(self.dataset), dtype=float)\n",
    "#         scores = np.array([len(self.chunk_entities[i].intersection(q_entities)) for i in range(len(self.dataset))], dtype=float)\n",
    "#         if scores.max() > 0:\n",
    "#             scores = scores / (scores.max() + 1e-12)\n",
    "#         return scores\n",
    "\n",
    "#     def graphrag_query(self, query: str, top_k: int = 5, alpha: float = 0.8):\n",
    "#         \"\"\"\n",
    "#         Combine semantic + graph signals.\n",
    "#         alpha: weight for semantic retrieval (0..1). final_score = alpha*sem + (1-alpha)*graph\n",
    "#         \"\"\"\n",
    "#         # semantic candidates\n",
    "#         sem = self._semantic_search(query, top_k=top_k * 2)  # retrieve more to allow reranking\n",
    "#         sem_idxs = [i for i, s in sem]\n",
    "#         sem_scores = {i: s for (i, s) in sem}\n",
    "\n",
    "#         # graph scores across dataset\n",
    "#         graph_scores = self._graph_search_scores(query)\n",
    "\n",
    "#         # compute combined score for sem candidates\n",
    "#         combined = []\n",
    "#         for i, sem_score in sem:\n",
    "#             gscore = graph_scores[i] if i < len(graph_scores) else 0.0\n",
    "#             score = alpha * sem_score + (1.0 - alpha) * gscore\n",
    "#             combined.append((i, score))\n",
    "\n",
    "#         # sort and pick top_k\n",
    "#         combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "#         combined_idxs = [i for i, s in combined_sorted]\n",
    "#         combined_chunks = [self.dataset[i] for i in combined_idxs]\n",
    "\n",
    "#         # gather also top graph-only chunks (if graph points to others not in sem candidates)\n",
    "#         top_graph_idx = np.argsort(graph_scores)[-top_k:][::-1]\n",
    "#         graph_chunks = [self.dataset[i] for i in top_graph_idx if graph_scores[i] > 0 and i not in combined_idxs]\n",
    "\n",
    "#         # KG facts extraction (neighbours of query entities)\n",
    "#         kg_facts = []\n",
    "#         q_doc = self.nlp(query)\n",
    "#         q_entities = set([ent.text.strip() for ent in q_doc.ents if ent.text.strip()])\n",
    "#         for qe in q_entities:\n",
    "#             if self.kg is not None and qe in self.kg:\n",
    "#                 # successors & predecessors (limit to a few)\n",
    "#                 for nb in list(self.kg.successors(qe))[:3]:\n",
    "#                     rel = self.kg.get_edge_data(qe, nb).get(\"relation\", \"related_to\")\n",
    "#                     kg_facts.append(f\"{qe} {rel} {nb}.\")\n",
    "#                 for nb in list(self.kg.predecessors(qe))[:3]:\n",
    "#                     rel = self.kg.get_edge_data(nb, qe).get(\"relation\", \"related_to\")\n",
    "#                     kg_facts.append(f\"{nb} {rel} {qe}.\")\n",
    "\n",
    "#         combined_context = \" \".join(combined_chunks + graph_chunks + kg_facts)\n",
    "#         # combined_context = \" \".join(combined_chunks[:3] + kg_facts[:4])\n",
    "            \n",
    "#         return {\"query\": query, \"context\": combined_context, \"sem_idxs\": combined_idxs}\n",
    "    \n",
    "    \n",
    "#     # ---------------- LLM Answer ----------------\n",
    "#     def answer_question_llm(\n",
    "#         self,\n",
    "#         query: str,\n",
    "#         llm_model: str = \"moonshotai/kimi-k2-instruct-0905\",\n",
    "#         top_k: int = 5,\n",
    "#         alpha: float = 0.8\n",
    "#     ):\n",
    "#         # result = self.graphrag_query(query, top_k=top_k, alpha=alpha)\n",
    "#         # context_text = result[\"context\"]\n",
    "#         # 1) GraphRAG retrieve\n",
    "#         result = self.graphrag_query(query, top_k=top_k, alpha=alpha)\n",
    "#         context_text = result[\"context\"]\n",
    "\n",
    "#         # 2) QWEN reasoning (local, SAT-trained)\n",
    "#         reasoning = qwen_reasoning(query, context_text)\n",
    "\n",
    "#         final_prompt = f\"\"\"\n",
    "# You are a high-accuracy QA assistant. \n",
    "\n",
    "# You MUST answer using ONLY the provided context.\n",
    "\n",
    "# Your output must follow these rules:\n",
    "\n",
    "# -------------------------------------------------\n",
    "# DETECTION RULES (auto-detect the question type):\n",
    "# - If the question asks for a specific name, place, person, object, event,\n",
    "#   ‚Üí classify as: FACT RETRIEVAL.\n",
    "# - If the question asks ‚Äúwhy‚Äù, ‚Äúhow‚Äù, ‚Äúwhat happened‚Äù, ‚Äúwhat was the relationship‚Äù,\n",
    "#   or requires combining multiple clues,\n",
    "#   ‚Üí classify as: COMPLEX REASONING.\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ANSWER STYLE RULES:\n",
    "# 1. FACT RETRIEVAL:\n",
    "#    - Answer in **3‚Äì12 words**.\n",
    "#    - Must be short, factual, and contain ONLY the requested entity.\n",
    "#    - Do NOT add explanations or context.\n",
    "\n",
    "# 2. COMPLEX REASONING:\n",
    "#    - Answer in 1‚Äì2 concise sentences (12‚Äì25 words total).\n",
    "#    - Combine only the essential evidence from the context.\n",
    "#    - No paraphrasing of long passages; no extra narrative.\n",
    "#    - The answer must be strictly grounded in the provided context.\n",
    "\n",
    "# 3. If the question_type is \"Contextual Summarize\":\n",
    "#    - Produce 2‚Äì4 sentences.\n",
    "#    - Length must be 80‚Äì200 words.\n",
    "#    - Provide a smooth, coherent summary combining *all* major points from context.\n",
    "#    - Use descriptive but concise language.\n",
    "#    - Do NOT shorten aggressively.\n",
    "#    - Do NOT reduce to a single-sentence summary.\n",
    "#    - This rule overrides any sentence-limit rules.\n",
    "\n",
    "# 4. If the question_type is \"Creative Generation\":\n",
    "#    - Follow the exact requested format (diary entry, newspaper article, retelling, etc.).\n",
    "#    - Stay grounded in the given evidence; do not introduce new events or characters.\n",
    "#    - 3‚Äì6 sentences depending on style, expressive but controlled.\n",
    "\n",
    "# 5. GENERAL RULES:\n",
    "#     - Do NOT add information not present in the context.\n",
    "#     - If context is incomplete, give the best-supported answer.\n",
    "#     - If no relevant information exists, reply EXACTLY:\n",
    "#     no relevant information found\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONTEXT:\n",
    "# {context_text}\n",
    "\n",
    "# QUESTION:\n",
    "# {query}\n",
    "\n",
    "# SAT REASONING:\n",
    "# {reasoning}\n",
    "\n",
    "# ANSWER:\n",
    "# \"\"\"\n",
    "#         # fix: ch·∫°y async function trong sync\n",
    "#         # import asyncio\n",
    "#         # answer = asyncio.run(llm_generate(prompt, model=llm_model))\n",
    "#         answer = llm_generate(final_prompt, model=llm_model)\n",
    "\n",
    "#         return answer, result\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # ---------------- Evaluation ----------------\n",
    "#     def evaluate_answer(self, pred: str, truth: str) -> float:\n",
    "#         scorer = Rouge()\n",
    "#         scores = scorer.get_scores(pred, truth)\n",
    "#         return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "#     # ---------------- Cache helpers ----------------\n",
    "#     def save_index(self, fname: str = \"faiss.index\"):\n",
    "#         if self.index is None:\n",
    "#             raise ValueError(\"No index to save.\")\n",
    "#         path = os.path.join(self.cfg.working_dir, fname)\n",
    "#         faiss.write_index(self.index, path)\n",
    "#         logger.info(f\"FAISS index written to {path}\")\n",
    "\n",
    "#     def load_index(self, fname: str = \"faiss.index\"):\n",
    "#         path = os.path.join(self.cfg.working_dir, fname)\n",
    "#         if not os.path.exists(path):\n",
    "#             raise FileNotFoundError(path)\n",
    "#         self.index = faiss.read_index(path)\n",
    "#         logger.info(f\"FAISS index loaded from {path}\")\n",
    "\n",
    "#     def save_embeddings(self, fname: str = \"embeddings.npy\"):\n",
    "#         if self.embeddings is None:\n",
    "#             raise ValueError(\"No embeddings to save.\")\n",
    "#         np.save(os.path.join(self.cfg.working_dir, fname), self.embeddings)\n",
    "#         logger.info(\"Embeddings saved.\")\n",
    "\n",
    "#     def load_embeddings(self, fname: str = \"embeddings.npy\"):\n",
    "#         path = os.path.join(self.cfg.working_dir, fname)\n",
    "#         if not os.path.exists(path):\n",
    "#             raise FileNotFoundError(path)\n",
    "#         self.embeddings = np.load(path)\n",
    "#         logger.info(\"Embeddings loaded.\")\n",
    "\n",
    "#     # ---------------- Convenience run -> inference on JSON questions ----------------\n",
    "# def run_inference_on_medical_json(rag, json_path, num_questions=10,question_type_filter=\"Fact Retrieval\"):\n",
    "#     \"\"\"\n",
    "#     Ch·∫°y LLM tr√™n c√°c c√¢u h·ªèi JSON ƒë∆∞·ª£c l·ªçc theo question_type.\n",
    "#     M·∫∑c ƒë·ªãnh ch·∫°y c√°c c√¢u 'Fact Retrieval'.\n",
    "#     \"\"\"\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "#     # L·ªçc c√°c c√¢u h·ªèi theo lo·∫°i y√™u c·∫ßu\n",
    "#     # L·ªçc c√°c c√¢u h·ªèi theo lo·∫°i y√™u c·∫ßu\n",
    "#     filtered = [\n",
    "#         item for item in data\n",
    "#         if item.get(\"question_type\", \"\").lower() == question_type_filter.lower()\n",
    "#     ]\n",
    "\n",
    "#     if not filtered:\n",
    "#         print(f\"‚ùå Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi lo·∫°i '{question_type_filter}' trong file: {json_path}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"üîπ Found {len(filtered)} questions of type '{question_type_filter}'\")\n",
    "#     print(f\"‚û° Running inference on first {num_questions}...\\n\")\n",
    "\n",
    "#     # print(f\"üîπ Running inference on {num_questions} questions from {json_path}\\n\")\n",
    "\n",
    "#     # ‚ùó USE filtered list, NOT full dataset\n",
    "#     for i, item in enumerate(filtered[:num_questions]):\n",
    "#         question = item.get(\"question\", \"\") or item.get(\"q\", \"\")\n",
    "#         ground_truth = item.get(\"answer\", \"\") or item.get(\"a\", \"\")\n",
    "#         # question_type = item.get(\"question_type\", \"\")\n",
    "#         # g·ªçi LLM\n",
    "#         ans, _ = rag.answer_question_llm(question)\n",
    "\n",
    "#         # x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi\n",
    "#         sentences = [s.strip() for s in ans.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "#         short_ans = \". \".join(sentences[:5]).strip()\n",
    "#         if short_ans and not short_ans.endswith(\".\"):\n",
    "#             short_ans += \".\"\n",
    "\n",
    "#         # in k·∫øt qu·∫£\n",
    "#         print(f\"=== Q{i+1}: {question}\")\n",
    "#         print(f\"Kimi K2 Answer:\\n{short_ans}\\n\")\n",
    "#         print(f\"Ground Truth:\\n{ground_truth}\\n\")\n",
    "#         print(\"-\" * 120)\n",
    "\n",
    "\n",
    "# # ---------------- Example usage ----------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # create rag with default config (you can pass a GraphRAGConfig instance to override)\n",
    "#     rag = SimpleGraphRAG(embedding_model_name=\"all-MiniLM-L6-v2\", chunk_size=1200, chunk_overlap=100)\n",
    "\n",
    "#     # Paths (change to your local dataset)\n",
    "#     corpus_path = \"/Users/duyanhle1501/Duy Anh Le Code/GraphRAG-Benchmark/Datasets/Corpus/medical.json\"\n",
    "#     questions_path = \"/Users/duyanhle1501/Duy Anh Le Code/GraphRAG-Benchmark/Datasets/Questions/medical_questions.json\"\n",
    "\n",
    "#     # 1) load corpus\n",
    "#     text = rag.load_json_and_concat(corpus_path)\n",
    "\n",
    "#     # 2) chunk (uses config tokenizer; default is tiktoken if available)\n",
    "#     rag.chunk_text(text, method=\"auto\")\n",
    "\n",
    "#     # 3) embedding + index\n",
    "#     rag.build_embeddings_and_index()\n",
    "\n",
    "#     # 4) build a proper KG (NER + simple dependency relations)\n",
    "#     rag.build_kg(text, use_dependency=True)\n",
    "\n",
    "#     # import pickle\n",
    "#     # import os\n",
    "\n",
    "#     # os.makedirs(\"sat_data\", exist_ok=True)\n",
    "\n",
    "#     # with open(\"sat_data/kg.pkl\", \"wb\") as f:\n",
    "#     #     pickle.dump(rag.kg, f)\n",
    "\n",
    "#     # print(\"‚úÖ Saved KG to sat_data/kg.pkl\")\n",
    "#     # print(\"Nodes:\", rag.kg.number_of_nodes())\n",
    "#     # print(\"Edges:\", rag.kg.number_of_edges())\n",
    "\n",
    "\n",
    "#     # 5) optional: cluster graph & embed nodes\n",
    "#     clusters = rag.cluster_graph()\n",
    "#     node_embs = rag.embed_graph_nodes()\n",
    "\n",
    "#     # 6) run inference on first 5 questions and print top 5-sentence summary\n",
    "#     run_inference_on_medical_json(rag, questions_path, num_questions=10, question_type_filter=\"Fact Retrieval\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒêo khi ƒë√£ c√≥ cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers faiss-cpu networkx nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NVAPI_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create a client wrapper\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVAPI_KEY\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast_graphrag_full.py\n",
    "\"\"\"\n",
    "Single-file FastGraphRAG (adapted).\n",
    "- Chunking: FastGraphRAG-like token-based chunking (800 tokens ‚âà 3200 chars).\n",
    "- IE: LLM-first extraction (Kimi wrapper), fallback heuristics.\n",
    "- Storage: minimal IGraphStorage (networkx-backed).\n",
    "- Embedding: sentence-transformers (optional), FAISS index (optional).\n",
    "- Main: insert corpus and answer first 5 questions from a JSON file.\n",
    "\n",
    "USAGE:\n",
    "  - Put your NVAPI key into an environment variable NVAPI_KEY in the notebook, e.g.:\n",
    "      import os\n",
    "      os.environ[\"NVAPI_KEY\"] = \"nvapi-....\"\n",
    "  - Provide a small async wrapper that calls your client and pass to KimiLLMService.\n",
    "  - Run the notebook cell to execute main.\n",
    "\n",
    "Do NOT hardcode API keys in this file.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Iterable, Awaitable\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from itertools import chain\n",
    "\n",
    "# Optional deps (guarded)\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except Exception:\n",
    "    nest_asyncio = None\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"FastGraphRAG_full\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "\n",
    "def run_async(coro):\n",
    "    \"\"\"Run coroutine safely both in notebooks and scripts.\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            if nest_asyncio is not None:\n",
    "                nest_asyncio.apply()\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return loop.run_until_complete(coro)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "# ----------------------------\n",
    "# Constants\n",
    "# ----------------------------\n",
    "TOKEN_TO_CHAR_RATIO = 4  # approximate: 1 token ‚âà 4 characters\n",
    "\n",
    "# ----------------------------\n",
    "# Embedding service (optional)\n",
    "# ----------------------------\n",
    "class BaseEmbeddingService:\n",
    "    async def aembed_documents(self, texts: List[str]) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def aembed_query(self, text: str) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SentenceEmbeddingService(BaseEmbeddingService):\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"Please install sentence-transformers to use SentenceEmbeddingService.\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    async def aembed_documents(self, texts: List[str]) -> np.ndarray:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: np.array(self.model.encode(texts, convert_to_numpy=True)))\n",
    "\n",
    "    async def aembed_query(self, text: str) -> np.ndarray:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: np.array(self.model.encode([text], convert_to_numpy=True)[0]))\n",
    "\n",
    "# ----------------------------\n",
    "# Kimi LLM wrapper (user provides completion_func)\n",
    "# ----------------------------\n",
    "class KimiLLMService:\n",
    "    \"\"\"\n",
    "    Wrap an async completion callable:\n",
    "      async def completion_func(prompt: str, **kwargs) -> str\n",
    "    The wrapper simply forwards prompt and returns the text response.\n",
    "    \"\"\"\n",
    "    def __init__(self, completion_func: Callable[[str], Awaitable[str]], model_name: str = \"moonshotai/kimi-k2-instruct-0905\"):\n",
    "        self.completion_func = completion_func\n",
    "        self.model = model_name\n",
    "\n",
    "    async def acomplete(self, prompt: str, **kwargs) -> str:\n",
    "        return await self.completion_func(prompt, **kwargs)\n",
    "\n",
    "# ----------------------------\n",
    "# FastGraphRAG config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class FastGraphRAGConfig:\n",
    "    working_dir: str = field(default_factory=lambda: f\"./fast_graphrag_cache_{np.random.randint(1e6)}\")\n",
    "    embedding_model_name: str = \"all-MiniLM-L6-v2\"\n",
    "    llm_model_name: str = \"moonshotai/kimi-k2-instruct-0905\"\n",
    "    chunk_token_size: int = 800\n",
    "    chunk_token_overlap: int = 100\n",
    "    # The above token-based values are converted to char approximations using TOKEN_TO_CHAR_RATIO.\n",
    "\n",
    "# ----------------------------\n",
    "# Chunking service (FastGraphRAG-like)\n",
    "# ----------------------------\n",
    "DEFAULT_SEPARATORS = [\"\\n\\n\\n\", \"\\n\\n\", \"\\r\\n\\r\\n\", \"„ÄÇ\", \"Ôºé\", \".\", \"ÔºÅ\", \"!\", \"Ôºü\", \"?\"]\n",
    "\n",
    "@dataclass\n",
    "class DefaultChunkingServiceConfig:\n",
    "    separators: List[str] = field(default_factory=lambda: DEFAULT_SEPARATORS)\n",
    "    chunk_token_size: int = 500\n",
    "    chunk_token_overlap: int = 100\n",
    "\n",
    "class DefaultChunkingService:\n",
    "    def __init__(self, cfg: Optional[DefaultChunkingServiceConfig] = None):\n",
    "        self.config = cfg or DefaultChunkingServiceConfig()\n",
    "        self._split_re = re.compile(f\"({'|'.join(re.escape(s) for s in self.config.separators or [])})\")\n",
    "        self._chunk_size = int(self.config.chunk_token_size * TOKEN_TO_CHAR_RATIO)\n",
    "        self._chunk_overlap = int(self.config.chunk_token_overlap * TOKEN_TO_CHAR_RATIO)\n",
    "\n",
    "    async def extract(self, documents: Iterable[str]) -> List[List[Dict[str, Any]]]:\n",
    "        \"\"\"Return list of list-of-chunks: [{'id': int, 'content': str, 'metadata': {}} ...]\"\"\"\n",
    "        out = []\n",
    "        for doc in documents:\n",
    "            chunks = await self._extract_chunks(doc)\n",
    "            seen = set()\n",
    "            uniq = []\n",
    "            for chunk in chunks:\n",
    "                # stable hash: use Python hash salted by content -> absolute into 63-bit\n",
    "                hid = abs(hash(chunk)) % (2**63)\n",
    "                if hid not in seen:\n",
    "                    seen.add(hid)\n",
    "                    uniq.append({\"id\": hid, \"content\": chunk, \"metadata\": {}})\n",
    "            out.append(uniq)\n",
    "        return out\n",
    "\n",
    "    async def _extract_chunks(self, text: str) -> List[str]:\n",
    "        if text is None:\n",
    "            return []\n",
    "        # sanitize\n",
    "        try:\n",
    "            text = text.encode(errors=\"replace\").decode()\n",
    "        except Exception:\n",
    "            text = re.sub(r\"[\\x00-\\x09\\x11-\\x12\\x14-\\x1f]\", \" \", text)\n",
    "\n",
    "        if len(text) <= self._chunk_size:\n",
    "            return [text]\n",
    "\n",
    "        parts = self._split_re.split(text)\n",
    "        parts.append(\"\")  # sentinel\n",
    "        merged: List[List[str]] = []\n",
    "        cur: List[str] = []\n",
    "        cur_len = 0\n",
    "        for i, p in enumerate(parts):\n",
    "            plen = len(p)\n",
    "            is_sep = (i % 2 == 1)\n",
    "            if is_sep or (cur_len + plen <= self._chunk_size - (self._chunk_overlap if len(merged) > 0 else 0)):\n",
    "                cur.append(p)\n",
    "                cur_len += plen\n",
    "            else:\n",
    "                merged.append(cur)\n",
    "                cur = [p]\n",
    "                cur_len = plen\n",
    "        if cur:\n",
    "            merged.append(cur)\n",
    "\n",
    "        # enforce overlap\n",
    "        out: List[str] = []\n",
    "        for idx, chunk_parts in enumerate(merged):\n",
    "            if idx == 0:\n",
    "                out.append(\"\".join(chunk_parts))\n",
    "            else:\n",
    "                prev = merged[idx - 1]\n",
    "                ov = []\n",
    "                ov_len = 0\n",
    "                for part in reversed(prev):\n",
    "                    if ov_len + len(part) > self._chunk_overlap:\n",
    "                        break\n",
    "                    ov.insert(0, part)\n",
    "                    ov_len += len(part)\n",
    "                out.append(\"\".join(ov + chunk_parts))\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Minimal IGraphStorage (networkx backed) ‚Äî simplified API\n",
    "# ----------------------------\n",
    "class IGraphStorage:\n",
    "    def __init__(self):\n",
    "        self._G = nx.DiGraph()\n",
    "        self._node_list: List[str] = []\n",
    "        self._edge_list: List[Tuple[str, str]] = []\n",
    "        self._in_progress = False\n",
    "\n",
    "    async def insert_start(self):\n",
    "        self._in_progress = True\n",
    "\n",
    "    async def insert_done(self):\n",
    "        # refresh lists for stable indexing\n",
    "        self._node_list = list(self._G.nodes())\n",
    "        self._edge_list = [(u, v) for u, v in self._G.edges()]\n",
    "        self._in_progress = False\n",
    "\n",
    "    async def node_count(self) -> int:\n",
    "        return self._G.number_of_nodes()\n",
    "\n",
    "    async def edge_count(self) -> int:\n",
    "        return self._G.number_of_edges()\n",
    "\n",
    "    async def add_entity(self, name: str, attrs: Optional[Dict[str, Any]] = None) -> int:\n",
    "        attrs = attrs or {}\n",
    "        if not self._G.has_node(name):\n",
    "            self._G.add_node(name, **attrs)\n",
    "        else:\n",
    "            self._G.nodes[name].update(attrs)\n",
    "        if name not in self._node_list:\n",
    "            self._node_list.append(name)\n",
    "        return self._node_list.index(name)\n",
    "\n",
    "    async def add_relation(self, source: str, target: str, attrs: Optional[Dict[str, Any]] = None) -> int:\n",
    "        attrs = attrs or {}\n",
    "        if not self._G.has_node(source):\n",
    "            await self.add_entity(source, {})\n",
    "        if not self._G.has_node(target):\n",
    "            await self.add_entity(target, {})\n",
    "        self._G.add_edge(source, target, **attrs)\n",
    "        self._edge_list.append((source, target))\n",
    "        return len(self._edge_list) - 1\n",
    "\n",
    "    async def get_node_by_index(self, idx: int) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            n = self._node_list[idx]\n",
    "            d = dict(self._G.nodes[n])\n",
    "            d[\"_name\"] = n\n",
    "            return d\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def get_edge_by_index(self, idx: int) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            s, t = self._edge_list[idx]\n",
    "            attrs = dict(self._G[s][t])\n",
    "            return {\"source\": s, \"target\": t, **attrs}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def are_neighbours(self, src_idx: int, tgt_idx: int) -> bool:\n",
    "        try:\n",
    "            s = self._node_list[src_idx]; t = self._node_list[tgt_idx]\n",
    "            return self._G.has_edge(s, t)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    async def get_entities_to_relationships_map(self):\n",
    "        rels = list(self._G.edges(data=True))\n",
    "        ent_to_rel = []\n",
    "        for node_idx, node in enumerate(self._node_list):\n",
    "            l = []\n",
    "            for rel_idx, (s, t, data) in enumerate(rels):\n",
    "                if s == node or t == node:\n",
    "                    l.append(rel_idx)\n",
    "            ent_to_rel.append(l)\n",
    "        return ent_to_rel\n",
    "\n",
    "    async def get_relationships_attrs(self, key: str = \"chunks\"):\n",
    "        rels = list(self._G.edges(data=True))\n",
    "        return [data.get(key, []) for _, _, data in rels]\n",
    "\n",
    "    async def save_graphml(self, output_path: str):\n",
    "        nx.write_graphml(self._G, output_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Information Extraction Service (LLM-first, 1-pass gleaning)\n",
    "# ----------------------------\n",
    "# ----------------------------\n",
    "# Information Extraction Service (no JSON required)\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class DefaultInformationExtractionService:\n",
    "    llm: KimiLLMService\n",
    "    # max_gleaning_steps kh√¥ng c·∫ßn n·ªØa, b·ªè ƒëi\n",
    "\n",
    "    async def extract_from_chunk(self, chunk_text: str, chunk_id: int) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Simple entity/relationship extraction without requiring JSON.\n",
    "        - Entities: capitalized phrases\n",
    "        - Relationships: co-occurrence in sentence\n",
    "        \"\"\"\n",
    "        ents = []\n",
    "        seen = set()\n",
    "        for m in re.finditer(r\"\\b([A-Z][a-z]{2,}(?:\\s+[A-Z][a-z]{2,})*)\\b\", chunk_text):\n",
    "            name = m.group(1).strip()\n",
    "            if name.lower() in seen:\n",
    "                continue\n",
    "            seen.add(name.lower())\n",
    "            ents.append({\"name\": name, \"type\": \"UNKNOWN\", \"desc\": \"\"})\n",
    "\n",
    "        rels = []\n",
    "        sents = re.split(r\"[.!?]\\s+\", chunk_text)\n",
    "        for sent in sents:\n",
    "            found = [e[\"name\"] for e in ents if e[\"name\"] in sent]\n",
    "            for i in range(len(found)):\n",
    "                for j in range(i+1, len(found)):\n",
    "                    rels.append({\n",
    "                        \"source\": found[i],\n",
    "                        \"target\": found[j],\n",
    "                        \"desc\": \"co_occurs_in_sentence\",\n",
    "                        \"chunks\": [chunk_id]\n",
    "                    })\n",
    "\n",
    "        return {\"entities\": ents, \"relationships\": rels}\n",
    "    #     prompt = f\"\"\"\n",
    "    # Extract important nouns and proper nouns from this text.\n",
    "    # Return them as a comma-separated list.\n",
    "    # TEXT: {chunk_text[:1500]}\n",
    "    # \"\"\"\n",
    "    #     raw = await self.llm.acomplete(prompt)\n",
    "    #     words = [w.strip() for w in raw.split(\",\") if w.strip()]\n",
    "\n",
    "    #     ents = []\n",
    "    #     for w in words:\n",
    "    #         ents.append({\"name\": w, \"type\": \"UNKNOWN\", \"desc\": \"\"})\n",
    "\n",
    "    #     # simple co-occurrence\n",
    "    #     rels = []\n",
    "    #     for i in range(len(words)):\n",
    "    #         for j in range(i+1, len(words)):\n",
    "    #             rels.append({\n",
    "    #                 \"source\": words[i],\n",
    "    #                 \"target\": words[j],\n",
    "    #                 \"desc\": \"co_occurs\",\n",
    "    #                 \"chunks\": [chunk_id]\n",
    "    #             })\n",
    "    #     return {\"entities\": ents, \"relationships\": rels}\n",
    "\n",
    "    async def gleaning(self, partial_graph: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        No LLM post-processing, just return partial_graph as-is\n",
    "        \"\"\"\n",
    "        return partial_graph\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main FastGraphRAG class\n",
    "# ----------------------------\n",
    "class FastGraphRAG:\n",
    "    def __init__(self, cfg: Optional[FastGraphRAGConfig] = None, embedding_service: Optional[BaseEmbeddingService] = None, llm_service: Optional[KimiLLMService] = None):\n",
    "        self.cfg = cfg or FastGraphRAGConfig()\n",
    "        os.makedirs(self.cfg.working_dir, exist_ok=True)\n",
    "\n",
    "        # embedding\n",
    "        if embedding_service is None:\n",
    "            if SentenceTransformer is not None:\n",
    "                self.embedding_service = SentenceEmbeddingService(self.cfg.embedding_model_name)\n",
    "            else:\n",
    "                self.embedding_service = None\n",
    "        else:\n",
    "            self.embedding_service = embedding_service\n",
    "\n",
    "        # llm\n",
    "        if llm_service is None:\n",
    "            raise RuntimeError(\"Please provide a KimiLLMService instance (completion wrapper).\")\n",
    "        self.llm_service = llm_service\n",
    "\n",
    "        # chunking & IE\n",
    "        chunk_cfg = DefaultChunkingServiceConfig(chunk_token_size=self.cfg.chunk_token_size, chunk_token_overlap=self.cfg.chunk_token_overlap)\n",
    "        self.chunking_service = DefaultChunkingService(chunk_cfg)\n",
    "        # self.ie_service = DefaultInformationExtractionService(llm=self.llm_service, max_gleaning_steps=1)\n",
    "        self.ie_service = DefaultInformationExtractionService(llm=self.llm_service)\n",
    "\n",
    "\n",
    "        # storage & index\n",
    "        self.kg = IGraphStorage()\n",
    "        self.chunks: List[str] = []\n",
    "        self.chunk_ids: List[int] = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "\n",
    "    async def async_insert(self, content: Any):\n",
    "        # load text\n",
    "        text = \"\"\n",
    "        if isinstance(content, str) and os.path.exists(content):\n",
    "            with open(content, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                text = \"\\n\".join([i.get(\"text\") or i.get(\"context\") or \"\" for i in data])\n",
    "            elif isinstance(data, dict):\n",
    "                text = data.get(\"text\") or data.get(\"context\") or \" \".join(map(str, data.values()))\n",
    "            else:\n",
    "                text = str(data)\n",
    "        else:\n",
    "            text = content if isinstance(content, str) else \" \".join(map(str, content))\n",
    "\n",
    "        # chunk using service\n",
    "        chunk_lists = await self.chunking_service.extract([text])\n",
    "        chunks = chunk_lists[0]\n",
    "        self.chunks = [c[\"content\"] for c in chunks]\n",
    "        self.chunk_ids = [c[\"id\"] for c in chunks]\n",
    "\n",
    "        # embeddings (if embedding service available)\n",
    "        if self.embedding_service is not None:\n",
    "            logger.info(\"Embedding %d chunks...\", len(self.chunks))\n",
    "            embs = await self.embedding_service.aembed_documents(self.chunks)\n",
    "            embs = embs / (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-9)\n",
    "            self.embeddings = embs.astype(\"float32\")\n",
    "            if faiss is not None:\n",
    "                import faiss as _faiss\n",
    "                self.index = _faiss.IndexFlatIP(self.embeddings.shape[1])\n",
    "                self.index.add(self.embeddings)\n",
    "                logger.info(\"Built FAISS index with %d vectors\", self.embeddings.shape[0])\n",
    "            else:\n",
    "                logger.warning(\"FAISS not installed; retrieval will fall back to substring search.\")\n",
    "                self.index = None\n",
    "        else:\n",
    "            logger.warning(\"No embedding service provided; retrieval will be substring-based.\")\n",
    "            self.index = None\n",
    "\n",
    "        # information extraction & build KG\n",
    "        await self.kg.insert_start()\n",
    "        try:\n",
    "            for i, chunk_text in enumerate(self.chunks):\n",
    "                res = await self.ie_service.extract_from_chunk(chunk_text, chunk_id=self.chunk_ids[i])\n",
    "                ents = res.get(\"entities\", []) or []\n",
    "                rels = res.get(\"relationships\", []) or []\n",
    "\n",
    "                for ent in ents:\n",
    "                    name = (ent.get(\"name\") or \"\").strip()\n",
    "                    if not name:\n",
    "                        continue\n",
    "                    attrs = {\"type\": ent.get(\"type\", \"\"), \"description\": ent.get(\"desc\", \"\")}\n",
    "                    await self.kg.add_entity(name, attrs)\n",
    "\n",
    "                for rel in rels:\n",
    "                    src = (rel.get(\"source\") or \"\").strip()\n",
    "                    tgt = (rel.get(\"target\") or \"\").strip()\n",
    "                    if not src or not tgt:\n",
    "                        continue\n",
    "                    attrs = {\"description\": rel.get(\"desc\", \"\"), \"chunks\": rel.get(\"chunks\", [self.chunk_ids[i]])}\n",
    "                    await self.kg.add_relation(src, tgt, attrs)\n",
    "        finally:\n",
    "            await self.kg.insert_done()\n",
    "\n",
    "        # one gleaning pass across the whole KG\n",
    "        partial = {\"entities\": [], \"relationships\": []}\n",
    "        for n in self.kg._node_list:\n",
    "            data = dict(self.kg._G.nodes[n])\n",
    "            partial[\"entities\"].append({\"name\": n, \"type\": data.get(\"type\", \"\"), \"desc\": data.get(\"description\", \"\")})\n",
    "        for (s, t, data) in self.kg._G.edges(data=True):\n",
    "            partial[\"relationships\"].append({\"source\": s, \"target\": t, \"desc\": data.get(\"description\", \"\"), \"chunks\": data.get(\"chunks\", [])})\n",
    "\n",
    "        updated = await self.ie_service.gleaning(partial)\n",
    "        new_ents = updated.get(\"entities\", [])\n",
    "        new_rels = updated.get(\"relationships\", [])\n",
    "        for ent in new_ents:\n",
    "            name = (ent.get(\"name\") or \"\").strip()\n",
    "            if name and name not in self.kg._G.nodes:\n",
    "                await self.kg.add_entity(name, {\"type\": ent.get(\"type\", \"\"), \"description\": ent.get(\"desc\", \"\")})\n",
    "        for rel in new_rels:\n",
    "            s = (rel.get(\"source\") or \"\").strip()\n",
    "            t = (rel.get(\"target\") or \"\").strip()\n",
    "            if s and t and not self.kg._G.has_edge(s, t):\n",
    "                await self.kg.add_relation(s, t, {\"description\": rel.get(\"desc\", \"\"), \"chunks\": rel.get(\"chunks\", [])})\n",
    "\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def insert(self, content: Any):\n",
    "        return run_async(self.async_insert(content))\n",
    "\n",
    "    # async def async_query(self, query: str, top_k: int = 5):\n",
    "    #     # retrieval\n",
    "    #     if self.index is not None and self.embedding_service is not None:\n",
    "    #         q_emb = await self.embedding_service.aembed_query(query)\n",
    "    #         D, I = self.index.search(np.array([q_emb], dtype=\"float32\"), top_k)\n",
    "    #         ctx = \" \".join([self.chunks[i] for i in I[0] if i < len(self.chunks)])\n",
    "    #     else:\n",
    "    #         # substring fallback: pick top_k chunks that contain most query tokens (simple heuristic)\n",
    "    #         tokens = re.findall(r\"\\w+\", query.lower())\n",
    "    #         scores = []\n",
    "    #         for c in self.chunks:\n",
    "    #             cnt = sum(1 for t in tokens if t in c.lower())\n",
    "    #             scores.append(cnt)\n",
    "    #         idxs = sorted(range(len(self.chunks)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    #         ctx = \" \".join([self.chunks[i] for i in idxs])\n",
    "\n",
    "    #     # prompt = (\n",
    "    #     #     \"You are a helpful assistant. Use ONLY the context below to answer the question. \"\n",
    "    #     #     \"If answer cannot be found, reply: \\\"no relevant information found\\\".\\n\\n\"\n",
    "    #     #     f\"CONTEXT:\\n{ctx}\\n\\nQUESTION:\\n{query}\\n\\n\"\n",
    "    #     # )\n",
    "    #     prompt = (\n",
    "    #         \"You are a concise QA assistant.\\n\"\n",
    "    # \"Your task is to answer the question using ONLY the provided context.\\n\\n\"\n",
    "    # \"RULES:\\n\"\n",
    "    # \"- The answer MUST be extremely short.\\n\"\n",
    "    # \"- Prefer a single word or a short noun phrase (1‚Äì3 words).\\n\"\n",
    "    # \"- Do NOT write full sentences.\\n\"\n",
    "    # \"- Do NOT add explanations.\\n\"\n",
    "    # \"- If the context suggests an answer, output ONLY the answer.\\n\"\n",
    "    # \"- If the answer is a name, output ONLY the name.\\n\"\n",
    "    # \"- If no answer exists in the context, reply exactly: no relevant information found.\\n\\n\"\n",
    "    # f\"CONTEXT:\\n{ctx}\\n\\n\"\n",
    "    # f\"QUESTION:\\n{query}\\n\\n\"\n",
    "    # \"ANSWER:\"\n",
    "    #     )\n",
    "\n",
    "    #     ans = await self.llm_service.acomplete(prompt)\n",
    "    #     return {\"response\": ans, \"context\": ctx}\n",
    "\n",
    "    \n",
    "    async def async_query(self, query: str, top_k: int = 10):\n",
    "    # -----------------------------\n",
    "    # 1) SEMANTIC RETRIEVAL\n",
    "    # -----------------------------\n",
    "        semantic_chunks = []\n",
    "        if self.index is not None and self.embedding_service is not None:\n",
    "            q_emb = await self.embedding_service.aembed_query(query)\n",
    "            D, I = self.index.search(np.array([q_emb], dtype=\"float32\"), top_k)\n",
    "            semantic_chunks = [self.chunks[i] for i in I[0] if i < len(self.chunks)]\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2) KEYWORD FALLBACK RETRIEVAL\n",
    "        # -----------------------------\n",
    "        tokens = re.findall(r\"\\w+\", query.lower())\n",
    "        keyword_scores = []\n",
    "        for idx, c in enumerate(self.chunks):\n",
    "            score = sum(1 for t in tokens if t in c.lower())\n",
    "            keyword_scores.append((score, idx))\n",
    "\n",
    "        keyword_scores.sort(reverse=True)\n",
    "        keyword_chunks = [self.chunks[idx] for score, idx in keyword_scores[:top_k]]\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3) MERGE + DEDUPLICATE CONTEXT\n",
    "        # -----------------------------\n",
    "        merged = semantic_chunks + keyword_chunks\n",
    "        ctx_list = []\n",
    "        seen = set()\n",
    "\n",
    "        for x in merged:\n",
    "            if x not in seen and len(x.strip()) > 0:\n",
    "                ctx_list.append(x)\n",
    "                seen.add(x)\n",
    "\n",
    "        ctx = \"\\n\\n\".join(ctx_list)\n",
    "\n",
    "        if not ctx.strip():\n",
    "            ctx = \"\\n\\n\".join(self.chunks[:min(3, len(self.chunks))])\n",
    "\n",
    "        # -----------------------------\n",
    "        # 4) ADAPTIVE QA PROMPT\n",
    "        # -----------------------------\n",
    "            # You are a high-quality QA assistant. \n",
    "    # You MUST answer using ONLY the provided context.\n",
    "\n",
    "    # ADAPTIVE ANSWER RULES:\n",
    "    # 1. If the answer is a simple fact (e.g., name, person, location, plant, medical term),\n",
    "    # respond in 3‚Äì12 words (not too short, not too long).\n",
    "\n",
    "    # 2. If the question requires reasoning, combining clues, or narrative interpretation,\n",
    "    # respond in **1‚Äì3 concise sentences**.\n",
    "\n",
    "    # 3. Never invent information. Never add details not present in the context.\n",
    "\n",
    "    # 4. If the context contains partial information, provide the best supported answer.\n",
    "\n",
    "    # 5. Only reply exactly: no relevant information found\n",
    "    # if the context truly contains zero clues.\n",
    "        prompt = f\"\"\"\n",
    "    \n",
    "    You are a high-accuracy QA assistant. \n",
    "\n",
    "You MUST answer using ONLY the provided context.\n",
    "\n",
    "Your output must follow these rules:\n",
    "\n",
    "-------------------------------------------------\n",
    "DETECTION RULES (auto-detect the question type):\n",
    "- If the question asks for a specific name, place, person, object, event,\n",
    "  ‚Üí classify as: FACT RETRIEVAL.\n",
    "- If the question asks ‚Äúwhy‚Äù, ‚Äúhow‚Äù, ‚Äúwhat happened‚Äù, ‚Äúwhat was the relationship‚Äù,\n",
    "  or requires combining multiple clues,\n",
    "  ‚Üí classify as: COMPLEX REASONING.\n",
    "-------------------------------------------------\n",
    "\n",
    "ANSWER STYLE RULES:\n",
    "1. FACT RETRIEVAL:\n",
    "   - Answer in **3‚Äì12 words**.\n",
    "   - Must be short, factual, and contain ONLY the requested entity.\n",
    "   - Do NOT add explanations or context.\n",
    "\n",
    "2. COMPLEX REASONING:\n",
    "   - Answer in 1‚Äì2 concise sentences (12‚Äì25 words total).\n",
    "   - Combine only the essential evidence from the context.\n",
    "   - No paraphrasing of long passages; no extra narrative.\n",
    "   - The answer must be strictly grounded in the provided context.\n",
    "\n",
    "3. If the question_type is \"Contextual Summarize\":\n",
    "   - Produce 2‚Äì4 sentences.\n",
    "   - Length must be 80‚Äì200 words.\n",
    "   - Provide a smooth, coherent summary combining *all* major points from context.\n",
    "   - Use descriptive but concise language.\n",
    "   - Do NOT shorten aggressively.\n",
    "   - Do NOT reduce to a single-sentence summary.\n",
    "   - This rule overrides any sentence-limit rules.\n",
    "\n",
    "4. If the question_type is \"Creative Generation\":\n",
    "   - Follow the exact requested format (diary entry, newspaper article, retelling, etc.).\n",
    "   - Stay grounded in the given evidence; do not introduce new events or characters.\n",
    "   - 3‚Äì6 sentences depending on style, expressive but controlled.\n",
    "\n",
    "5. GENERAL RULES:\n",
    "    - Do NOT add information not present in the context.\n",
    "    - If context is incomplete, give the best-supported answer.\n",
    "    - If no relevant information exists, reply EXACTLY:\n",
    "    no relevant information found\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "    CONTEXT:\n",
    "    {ctx}\n",
    "\n",
    "    QUESTION:\n",
    "    {query}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 5) LLM Call\n",
    "        # -----------------------------\n",
    "        ans = await self.llm_service.acomplete(prompt)\n",
    "        if ans:\n",
    "            ans = ans.strip()\n",
    "\n",
    "        # -----------------------------\n",
    "        # 6) ADAPTIVE POST-PROCESSING\n",
    "        # -----------------------------\n",
    "        if ans:\n",
    "            # N·∫øu c√¢u qu√° d√†i (> 120 t·ª´) ‚Üí r√∫t xu·ªëng 3 c√¢u\n",
    "            if len(ans.split()) > 120:\n",
    "                sentences = re.split(r\"[.!?]\", ans)\n",
    "                ans = \". \".join(sentences[:3]).strip()\n",
    "\n",
    "            # N·∫øu m√¥ h√¨nh tr·∫£ l·ªùi 1 t·ª´ qu√° ng·∫Øn cho fact retrieval\n",
    "            # ‚Üí KH√îNG c·∫Øt l·∫°i (v√¨ prompt ƒë√£ ƒë·∫£m b·∫£o 3‚Äì12 words)\n",
    "            # N√™n kh√¥ng l√†m g√¨ ·ªü ƒë√¢y\n",
    "\n",
    "        return {\"response\": ans, \"context\": ctx}\n",
    "\n",
    "\n",
    "\n",
    "    def query(self, query: str, top_k: int = 10):\n",
    "        return run_async(self.async_query(query, top_k))\n",
    "\n",
    "# ----------------------------\n",
    "# Demo runner & main\n",
    "# ----------------------------\n",
    "def run_inference_on_json(rag: FastGraphRAG, json_path: str, num_questions: int = 20):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for i, item in enumerate(data[:num_questions]):\n",
    "        q = item.get(\"question\", \"\") or item.get(\"query\", \"\")\n",
    "        gt = item.get(\"answer\", \"\") or item.get(\"gt\", \"\") or \"\"\n",
    "        print(f\"QUESTION {i+1} \")\n",
    "        res = rag.query(q, top_k=5)\n",
    "        print(\"LLM ANSWER:\", res[\"response\"])\n",
    "        print(\"\\nGROUND TRUTH:\", gt)\n",
    "        # print(\"-------------------------\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---------- USER ACTIONS ----------\n",
    "    # 1) Install dependencies in your notebook env:\n",
    "    #    pip install sentence-transformers faiss-cpu networkx nest_asyncio\n",
    "    #    (faiss-cpu optional; if absent you'll get substring fallback retrieval)\n",
    "    #\n",
    "    # 2) Set environment variable NVAPI_KEY in the notebook BEFORE running this file:\n",
    "    #    import os\n",
    "    #    os.environ[\"NVAPI_KEY\"] = \"nvapi-....\"   # <--- put your key here (do NOT commit)\n",
    "    #\n",
    "    # 3) Create a client wrapper (example below uses the OpenAI-like client you mentioned)\n",
    "    #\n",
    "    # ---------- wiring example ----------\n",
    "    from openai import OpenAI  # or the client SDK you have installed that matches the snippet\n",
    "    # Do NOT hardcode key in the file; get it from env\n",
    "    client = OpenAI(base_url=\"https://integrate.api.nvidia.com/v1\", api_key=os.getenv(\"NVAPI_KEY\"))\n",
    "\n",
    "    # async wrapper around the sync client call (runs in executor)\n",
    "    async def kimi_completion(prompt: str, **kwargs) -> str:\n",
    "        def call():\n",
    "            # Use your client to create a chat completion - adjust to your client's response shape\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=1024,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            # adapt to your SDK's return format; this assumes resp.choices[0].message.content\n",
    "            try:\n",
    "                return resp.choices[0].message.content\n",
    "            except Exception:\n",
    "                # fallback: str(resp)\n",
    "                return str(resp)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, call)\n",
    "\n",
    "    # Create LLM wrapper\n",
    "    kimi_service = KimiLLMService(kimi_completion)\n",
    "\n",
    "    # Create embedding service (optional)\n",
    "    if SentenceTransformer is not None:\n",
    "        emb_service = SentenceEmbeddingService(\"all-MiniLM-L6-v2\")\n",
    "    else:\n",
    "        emb_service = None\n",
    "\n",
    "    # Instantiate RAG\n",
    "    cfg = FastGraphRAGConfig()\n",
    "    rag = FastGraphRAG(cfg=cfg, embedding_service=emb_service, llm_service=kimi_service)\n",
    "\n",
    "    # Paths (update to your dataset paths)\n",
    "    corpus_path = \"/Users/duyanhle1501/Duy Anh Le Code/GraphRAG-Benchmark/Datasets/Corpus/novel.json\"\n",
    "    questions_path = \"/Users/duyanhle1501/Duy Anh Le Code/GraphRAG-Benchmark/Datasets/Questions/novel_questions.json\"\n",
    "\n",
    "    print(\"Inserting corpus (this may take some time)...\")\n",
    "    rag.insert(corpus_path)\n",
    "    print(\"Done inserting.\")\n",
    "\n",
    "    print(\"Running first 5 questions from questions file...\")\n",
    "    run_inference_on_json(rag, questions_path, num_questions=20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
